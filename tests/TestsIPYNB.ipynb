{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from types import SimpleNamespace\n",
    "import tensor2tensor\n",
    "from tensor2tensor.layers import common_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utilities for creating Sparsely-Gated Mixture-of-Experts Layers.\n",
    "See \"Outrageously Large Neural Networks\"\n",
    "https://arxiv.org/abs/1701.06538\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "import math\n",
    "import six\n",
    "from six.moves import range  # pylint: disable=redefined-builtin\n",
    "from six.moves import zip  # pylint: disable=redefined-builtin\n",
    "\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.layers.vq_discrete import DiscreteBottleneck\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "DEFAULT_DEV_STRING = \"existing_device\"\n",
    "\n",
    "\n",
    "def add_scope(scope=None, scope_fn=None):\n",
    "  \"\"\"Return a decorator which add a TF name/variable scope to a function.\n",
    "  Note that the function returned by the decorator accept an additional 'name'\n",
    "  parameter, which can overwrite the name scope given when the function is\n",
    "  created.\n",
    "  Args:\n",
    "    scope (str): name of the scope. If None, the function name is used.\n",
    "    scope_fn (fct): Either tf.name_scope or tf.variable_scope\n",
    "  Returns:\n",
    "    fct: the add_scope decorator\n",
    "  \"\"\"\n",
    "  def decorator(f):\n",
    "\n",
    "    @functools.wraps(f)\n",
    "    def decorated(*args, **kwargs):\n",
    "      name = kwargs.pop(\"name\", None)  # Python 2 hack for keyword only args\n",
    "      with scope_fn(name or scope or f.__name__):\n",
    "        return f(*args, **kwargs)\n",
    "\n",
    "    return decorated\n",
    "\n",
    "  return decorator\n",
    "\n",
    "\n",
    "def add_var_scope(scope=None):\n",
    "  return add_scope(scope, scope_fn=tf.compat.v1.variable_scope)\n",
    "\n",
    "\n",
    "def add_name_scope(scope=None):\n",
    "  return add_scope(scope, scope_fn=tf.name_scope)\n",
    "\n",
    "\n",
    "def _add_variable_proxy_methods(var, proxy_tensor):\n",
    "  \"\"\"Proxy methods of underlying variable.\n",
    "  This enables our custom getters to still work with, e.g., batch norm.\n",
    "  Args:\n",
    "    var: Variable to proxy\n",
    "    proxy_tensor: Tensor that is identity of var\n",
    "  \"\"\"\n",
    "  proxy_tensor.read_value = lambda: tf.identity(proxy_tensor)\n",
    "  proxy_tensor.assign_sub = var.assign_sub\n",
    "  proxy_tensor.assign = var.assign\n",
    "  proxy_tensor.initialized_value = var.initialized_value\n",
    "\n",
    "\n",
    "class Parallelism(object):\n",
    "  \"\"\"Helper class for creating sets of parallel function calls.\n",
    "  The purpose of this class is to replace this code:\n",
    "      e = []\n",
    "      f = []\n",
    "      for i in range(len(devices)):\n",
    "        with tf.device(devices[i]):\n",
    "          e_, f_ = func(a[i], b[i], c)\n",
    "          e.append(e_)\n",
    "          f.append(f_)\n",
    "  with this code:\n",
    "      e, f = expert_utils.Parallelism(devices)(func, a, b, c)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               device_names_or_functions,\n",
    "               reuse=True,\n",
    "               caching_devices=None,\n",
    "               daisy_chain_variables=False,\n",
    "               ps_devices=None):\n",
    "    \"\"\"Create a Parallelism.\n",
    "    Args:\n",
    "      device_names_or_functions: A list of length n, containing device names\n",
    "        or device functions (see `tf.device`)\n",
    "      reuse: True or None.  Whether to reuse variables created in the first\n",
    "        replica in the subsequent replicas.\n",
    "      caching_devices: Either `None`, or a list of length n containing device\n",
    "        names.\n",
    "      daisy_chain_variables: a boolean - if true, then copies variables in a\n",
    "        daisy chain between devices.\n",
    "      ps_devices: list<str>, list of devices for experts.\n",
    "    Returns:\n",
    "      a Parallelism.\n",
    "    \"\"\"\n",
    "    assert device_names_or_functions\n",
    "    self._devices = device_names_or_functions\n",
    "    self._n = len(device_names_or_functions)\n",
    "    self._reuse = reuse\n",
    "    self._caching_devices = self._maybe_repeat(caching_devices)\n",
    "    self._daisy_chain_variables = daisy_chain_variables\n",
    "    self._ps_devices = ps_devices or [\"\"]\n",
    "\n",
    "  def __call__(self, fn, *args, **kwargs):\n",
    "    \"\"\"A parallel set of function calls (using the specified devices).\n",
    "    Args:\n",
    "      fn: a function or a list of n functions.\n",
    "      *args: additional args.  Each arg should either be not a list, or a list\n",
    "         of length n.\n",
    "      **kwargs: additional keyword args.  Each arg should either be not a\n",
    "         list, or a list of length n.\n",
    "    Returns:\n",
    "      either a single list of length n (if fn does not return a tuple), or a\n",
    "      tuple of lists of length n (if fn returns a tuple).\n",
    "    \"\"\"\n",
    "    # Construct lists or args and kwargs for each function.\n",
    "    if args:\n",
    "      my_args = transpose_list_of_lists(\n",
    "          [self._maybe_repeat(arg) for arg in args])\n",
    "    else:\n",
    "      my_args = [[] for _ in range(self.n)]\n",
    "    my_kwargs = [{} for _ in range(self.n)]\n",
    "    for k, v in six.iteritems(kwargs):\n",
    "      vals = self._maybe_repeat(v)\n",
    "      for i in range(self.n):\n",
    "        my_kwargs[i][k] = vals[i]\n",
    "\n",
    "    # Construct lists of functions.\n",
    "    fns = self._maybe_repeat(fn)\n",
    "\n",
    "    # Now make the parallel call.\n",
    "    outputs = []\n",
    "    cache = {}\n",
    "    tensor_to_var = {}\n",
    "    for i in range(self.n):\n",
    "\n",
    "      def daisy_chain_getter(getter, name, *args, **kwargs):\n",
    "        \"\"\"Get a variable and cache in a daisy chain.\"\"\"\n",
    "        device_var_key = (self._devices[i], name)\n",
    "        if device_var_key in cache:\n",
    "          # if we have the variable on the correct device, return it.\n",
    "          return cache[device_var_key]\n",
    "        if name in cache:\n",
    "          # if we have it on a different device, copy it from the last device\n",
    "          last_device_v = cache[name]\n",
    "          var = tensor_to_var[last_device_v]\n",
    "          v = tf.identity(last_device_v)\n",
    "        else:\n",
    "          var = getter(name, *args, **kwargs)\n",
    "          v = var.read_value()\n",
    "\n",
    "        # keep track of the original variable\n",
    "        tensor_to_var[v] = var\n",
    "        _add_variable_proxy_methods(tensor_to_var[v], v)\n",
    "        # update the cache\n",
    "        cache[name] = v\n",
    "        cache[device_var_key] = v\n",
    "        return v\n",
    "\n",
    "      # Variable scope will not reset caching_device on reused variables,\n",
    "      # so we make a custom getter that uses identity to cache the variable.\n",
    "      # pylint: disable=cell-var-from-loop\n",
    "      def caching_getter(getter, name, *args, **kwargs):\n",
    "        \"\"\"Cache variables on device.\"\"\"\n",
    "        key = (self._caching_devices[i], name)\n",
    "        if key in cache:\n",
    "          return cache[key]\n",
    "\n",
    "        v = getter(name, *args, **kwargs)\n",
    "        with tf.device(self._caching_devices[i]):\n",
    "          ret = v.read_value()\n",
    "        _add_variable_proxy_methods(v, ret)\n",
    "        cache[key] = ret\n",
    "        return ret\n",
    "\n",
    "      if self._daisy_chain_variables:\n",
    "        custom_getter = daisy_chain_getter\n",
    "      elif self._caching_devices[i]:\n",
    "        custom_getter = caching_getter\n",
    "      else:\n",
    "        custom_getter = None\n",
    "      # pylint: enable=cell-var-from-loop\n",
    "      with tf.name_scope(\"parallel_%d\" % i):\n",
    "        with tf.variable_scope(\n",
    "            tf.get_variable_scope() if self._reuse else \"parallel_%d\" % i,\n",
    "            reuse=True if i > 0 and self._reuse else None,\n",
    "            caching_device=self._caching_devices[i],\n",
    "            custom_getter=custom_getter):\n",
    "          # TODO(noam, epot, avaswani)\n",
    "          # Allows for passing no device in case you want to default to the\n",
    "          # existing device. This is needed when we put all experts on a single\n",
    "          # device, for example in local_moe.\n",
    "          if self._devices[i] != DEFAULT_DEV_STRING:\n",
    "            with tf.device(self._devices[i]):\n",
    "              outputs.append(fns[i](*my_args[i], **my_kwargs[i]))\n",
    "          else:\n",
    "            outputs.append(fns[i](*my_args[i], **my_kwargs[i]))\n",
    "    if isinstance(outputs[0], tuple):\n",
    "      outputs = list(zip(*outputs))\n",
    "      outputs = tuple([list(o) for o in outputs])\n",
    "    return outputs\n",
    "\n",
    "  @property\n",
    "  def n(self):\n",
    "    return self._n\n",
    "\n",
    "  @property\n",
    "  def devices(self):\n",
    "    return self._devices\n",
    "\n",
    "  @property\n",
    "  def ps_devices(self):\n",
    "    return self._ps_devices\n",
    "\n",
    "  def _maybe_repeat(self, x):\n",
    "    \"\"\"Utility function for processing arguments that are singletons or lists.\n",
    "    Args:\n",
    "      x: either a list of self.n elements, or not a list.\n",
    "    Returns:\n",
    "      a list of self.n elements.\n",
    "    \"\"\"\n",
    "    if isinstance(x, list):\n",
    "      assert len(x) == self.n\n",
    "      return x\n",
    "    else:\n",
    "      return [x] * self.n\n",
    "\n",
    "\n",
    "def _rowwise_unsorted_segment_sum(values, indices, n):\n",
    "  \"\"\"UnsortedSegmentSum on each row.\n",
    "  Args:\n",
    "    values: a `Tensor` with shape `[batch_size, k]`.\n",
    "    indices: an integer `Tensor` with shape `[batch_size, k]`.\n",
    "    n: an integer.\n",
    "  Returns:\n",
    "    A `Tensor` with the same type as `values` and shape `[batch_size, n]`.\n",
    "  \"\"\"\n",
    "  batch, k = tf.unstack(tf.shape(indices), num=2)\n",
    "  indices_flat = tf.reshape(indices, [-1]) + tf.div(tf.range(batch * k), k) * n\n",
    "  ret_flat = tf.unsorted_segment_sum(\n",
    "      tf.reshape(values, [-1]), indices_flat, batch * n)\n",
    "  return tf.reshape(ret_flat, [batch, n])\n",
    "\n",
    "\n",
    "def _normal_distribution_cdf(x, stddev):\n",
    "  \"\"\"Evaluates the CDF of the normal distribution.\n",
    "  Normal distribution with mean 0 and standard deviation stddev,\n",
    "  evaluated at x=x.\n",
    "  input and output `Tensor`s have matching shapes.\n",
    "  Args:\n",
    "    x: a `Tensor`\n",
    "    stddev: a `Tensor` with the same shape as `x`.\n",
    "  Returns:\n",
    "    a `Tensor` with the same shape as `x`.\n",
    "  \"\"\"\n",
    "  return 0.5 * (1.0 + tf.erf(x / (math.sqrt(2) * stddev + 1e-20)))\n",
    "\n",
    "\n",
    "def _prob_in_top_k(\n",
    "    clean_values, noisy_values, noise_stddev, noisy_top_values, k):\n",
    "  \"\"\"Helper function to NoisyTopKGating.\n",
    "  Computes the probability that value is in top k, given different random noise.\n",
    "  This gives us a way of backpropagating from a loss that balances the number\n",
    "  of times each expert is in the top k experts per example.\n",
    "  In the case of no noise, pass in None for noise_stddev, and the result will\n",
    "  not be differentiable.\n",
    "  Args:\n",
    "    clean_values: a `Tensor` of shape [batch, n].\n",
    "    noisy_values: a `Tensor` of shape [batch, n].  Equal to clean values plus\n",
    "      normally distributed noise with standard deviation noise_stddev.\n",
    "    noise_stddev: a `Tensor` of shape [batch, n], or None\n",
    "    noisy_top_values: a `Tensor` of shape [batch, m].\n",
    "       \"values\" Output of tf.top_k(noisy_top_values, m).  m >= k+1\n",
    "    k: an integer.\n",
    "  Returns:\n",
    "    a `Tensor` of shape [batch, n].\n",
    "  \"\"\"\n",
    "  batch = tf.shape(clean_values)[0]\n",
    "  m = tf.shape(noisy_top_values)[1]\n",
    "  top_values_flat = tf.reshape(noisy_top_values, [-1])\n",
    "  # we want to compute the threshold that a particular value would have to\n",
    "  # exceed in order to make the top k.  This computation differs depending\n",
    "  # on whether the value is already in the top k.\n",
    "  threshold_positions_if_in = tf.range(batch) * m + k\n",
    "  threshold_if_in = tf.expand_dims(\n",
    "      tf.gather(top_values_flat, threshold_positions_if_in), 1)\n",
    "  is_in = tf.greater(noisy_values, threshold_if_in)\n",
    "  if noise_stddev is None:\n",
    "    return tf.to_float(is_in)\n",
    "  threshold_positions_if_out = threshold_positions_if_in - 1\n",
    "  threshold_if_out = tf.expand_dims(\n",
    "      tf.gather(top_values_flat, threshold_positions_if_out), 1)\n",
    "  # is each value currently in the top k.\n",
    "  prob_if_in = _normal_distribution_cdf(clean_values - threshold_if_in,\n",
    "                                        noise_stddev)\n",
    "  prob_if_out = _normal_distribution_cdf(clean_values - threshold_if_out,\n",
    "                                         noise_stddev)\n",
    "  prob = tf.where(is_in, prob_if_in, prob_if_out)\n",
    "  return prob\n",
    "\n",
    "\n",
    "def cv_squared(x):\n",
    "  \"\"\"The squared coefficient of variation of a sample.\n",
    "  Useful as a loss to encourage a positive distribution to be more uniform.\n",
    "  Epsilons added for numerical stability.\n",
    "  Returns 0 for an empty Tensor.\n",
    "  Args:\n",
    "    x: a `Tensor`.\n",
    "  Returns:\n",
    "    a `Scalar`.\n",
    "  \"\"\"\n",
    "  epsilon = 1e-10\n",
    "  float_size = tf.to_float(tf.size(x)) + epsilon\n",
    "  mean = tf.reduce_sum(x) / float_size\n",
    "  variance = tf.reduce_sum(tf.squared_difference(x, mean)) / float_size\n",
    "  return variance / (tf.square(mean) + epsilon)\n",
    "\n",
    "\n",
    "def _gates_to_load(gates):\n",
    "  \"\"\"Compute the true load per expert, given the gates.\n",
    "  The load is the number of examples for which the corresponding gate is >0.\n",
    "  Args:\n",
    "    gates: a `Tensor` of shape [batch_size, n]\n",
    "  Returns:\n",
    "    a float32 `Tensor` of shape [n]\n",
    "  \"\"\"\n",
    "  return tf.reduce_sum(tf.to_float(gates > 0), 0)\n",
    "\n",
    "\n",
    "def update_hparams_for_vq_gating(hparams):\n",
    "  \"\"\"VQ Gating hparams.\"\"\"\n",
    "  hparams.add_hparam(\"z_size\", 4)\n",
    "  hparams.add_hparam(\"noise_dev\", 0.5)\n",
    "  # Bottleneck kinds supported: dense, vae, dvq.\n",
    "  hparams.add_hparam(\"bottleneck_kind\", \"dvq\")\n",
    "  hparams.add_hparam(\"num_blocks\", 1)\n",
    "  hparams.add_hparam(\"num_residuals\", 1)\n",
    "  # Reshape method for DVQ: slice, project\n",
    "  hparams.add_hparam(\"beta\", 0.25)\n",
    "  hparams.add_hparam(\"epsilon\", 1e-5)\n",
    "  hparams.add_hparam(\"decay\", 0.999)\n",
    "  hparams.add_hparam(\"ema\", False)  # default is false until ema is implemented\n",
    "  hparams.add_hparam(\"random_top_k\", 1)\n",
    "  hparams.add_hparam(\"soft_em\", False)\n",
    "  hparams.add_hparam(\"num_samples\", 10)\n",
    "  hparams.add_hparam(\"gating_type\", \"vq\")\n",
    "  hparams.add_hparam(\"use_scales\", int(True))\n",
    "  hparams.add_hparam(\"residual_centroids\", int(False))\n",
    "\n",
    "\n",
    "def _my_top_k(x, k):\n",
    "  \"\"\"GPU-compatible version of top-k that works for very small constant k.\n",
    "  Calls argmax repeatedly.\n",
    "  tf.nn.top_k is implemented for GPU, but the gradient, sparse_to_dense,\n",
    "  seems not to be, so if we use tf.nn.top_k, then both the top_k and its\n",
    "  gradient go on cpu.  Once this is not an issue, this function becomes\n",
    "  obsolete and should be replaced by tf.nn.top_k.\n",
    "  Args:\n",
    "    x: a 2d Tensor.\n",
    "    k: a small integer.\n",
    "  Returns:\n",
    "    values: a Tensor of shape [batch_size, k]\n",
    "    indices: a int32 Tensor of shape [batch_size, k]\n",
    "  \"\"\"\n",
    "  if k > 10:\n",
    "    return tf.nn.top_k(x, k)\n",
    "  values = []\n",
    "  indices = []\n",
    "  depth = tf.shape(x)[1]\n",
    "  for i in range(k):\n",
    "    values.append(tf.reduce_max(x, 1))\n",
    "    argmax = tf.argmax(x, 1)\n",
    "    indices.append(argmax)\n",
    "    if i + 1 < k:\n",
    "      x += tf.one_hot(argmax, depth, -1e9)\n",
    "  return tf.stack(values, axis=1), tf.to_int32(tf.stack(indices, axis=1))\n",
    "\n",
    "\n",
    "def vq_gating(x,\n",
    "              num_experts,\n",
    "              k,\n",
    "              bneck,\n",
    "              hparams=None,\n",
    "              name=\"vq_gating\"):\n",
    "  \"\"\"VQ gating.\n",
    "  Args:\n",
    "    x: input Tensor with shape [batch_size, input_size]\n",
    "    num_experts: an integer\n",
    "    k: an integer - number of experts per example\n",
    "    bneck: a bottleneck object\n",
    "    hparams: optional hparams\n",
    "    name: an optional string\n",
    "  Returns:\n",
    "    gates: a Tensor with shape [batch_size, num_experts]\n",
    "    load: a Tensor with shape [num_experts]\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "\n",
    "    if hparams.use_scales:\n",
    "      scales = tf.get_variable(\n",
    "          \"scales\", [num_experts],\n",
    "          tf.float32,\n",
    "          initializer=tf.ones_initializer())\n",
    "      scales = tf.nn.softmax(scales)\n",
    "      hparams.scales = scales\n",
    "    input_size = x.get_shape().as_list()[-1]\n",
    "    batch_size = common_layers.shape_list(x)[0]\n",
    "\n",
    "    if k > 1:\n",
    "      # first project into two dense layers, chop and discretize, and gate\n",
    "      # TODO(avaswani): Maybe scale the embeddings flowing out of the experts.\n",
    "      # We might want to do this to match the computation being done by topk\n",
    "      x = tf.layers.dense(x, input_size * k)\n",
    "      # x goes from [batch_size, input_size*k] to [batch_size*k, input_size]\n",
    "      x = tf.reshape(x, [batch_size * k, input_size])\n",
    "    inputs = tf.expand_dims(x, axis=1)\n",
    "    inputs = tf.expand_dims(inputs, axis=1)\n",
    "    # VQ hparams\n",
    "    hparams.z_size = int(math.log(num_experts, 2))\n",
    "    hparams.hidden_size = input_size\n",
    "    hparams.top_k = k\n",
    "    d = bneck.discrete_bottleneck(inputs)\n",
    "    centroids = None\n",
    "    exp_discrete = d[\"discrete\"]\n",
    "    embed_lookup = d[\"embed\"]\n",
    "    extra_loss = d[\"loss\"]\n",
    "    if hparams.residual_centroids:\n",
    "      centroids = embed_lookup(exp_discrete)  # gives the centroids\n",
    "    top_k_indices = tf.squeeze(exp_discrete, axis=1)\n",
    "    tf.summary.histogram(\"discrete_counts\", top_k_indices)\n",
    "    # if k > 1, then we need to reshape top_k_indices from [batch_size*k, 1]\n",
    "    # to [batch_size, k]\n",
    "    if k > 1:\n",
    "      top_k_indices = tf.reshape(top_k_indices, [batch_size, k])\n",
    "    # get the top k gates\n",
    "    top_k_gates = tf.ones([batch_size, k])\n",
    "    # This will be a `Tensor` of shape `[batch_size, n]`, with zeros in the\n",
    "    # positions corresponding to all but the top k experts per example.\n",
    "    gates = _rowwise_unsorted_segment_sum(top_k_gates, top_k_indices,\n",
    "                                          num_experts)\n",
    "    # Compute count per expert from the gates.\n",
    "    # gates has shape [batch_size, num_experts]\n",
    "    # count per expert has shape [num_experts, 1]\n",
    "    count_per_expert = tf.reduce_sum(gates, axis=0)\n",
    "    if hparams.use_scales:\n",
    "      scale_loss = tf.reduce_mean(tf.to_float(count_per_expert) * scales)\n",
    "      extra_loss += scale_loss\n",
    "    if common_layers.should_generate_summaries():\n",
    "      tf.summary.histogram(\"vq_loss\", extra_loss)\n",
    "      tf.summary.historgram(\"scale_loss\", scale_loss)\n",
    "    return gates, extra_loss, centroids\n",
    "\n",
    "\n",
    "def noisy_top_k_gating(x,\n",
    "                       num_experts,\n",
    "                       train,\n",
    "                       k=2,\n",
    "                       initializer=tf.zeros_initializer(),\n",
    "                       noisy_gating=True,\n",
    "                       noise_epsilon=1e-2,\n",
    "                       name=None):\n",
    "  \"\"\"Noisy top-k gating.\n",
    "  See paper: https://arxiv.org/abs/1701.06538.\n",
    "  Args:\n",
    "    x: input Tensor with shape [batch_size, input_size]\n",
    "    num_experts: an integer\n",
    "    train: a boolean - we only add noise at training time.\n",
    "    k: an integer - number of experts per example\n",
    "    initializer: an initializer\n",
    "    noisy_gating: a boolean\n",
    "    noise_epsilon: a float\n",
    "    name: an optional string\n",
    "  Returns:\n",
    "    gates: a Tensor with shape [batch_size, num_experts]\n",
    "    load: a Tensor with shape [num_experts]\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(name, default_name=\"noisy_top_k_gating\"):\n",
    "    input_size = x.get_shape().as_list()[-1]\n",
    "    w_gate = tf.get_variable(\n",
    "        \"w_gate\", [input_size, num_experts], tf.float32, initializer)\n",
    "    if noisy_gating:\n",
    "      w_noise = tf.get_variable(\"w_noise\",\n",
    "                                [input_size, num_experts], tf.float32,\n",
    "                                initializer)\n",
    "    clean_logits = tf.matmul(x, w_gate)\n",
    "    if noisy_gating:\n",
    "      raw_noise_stddev = tf.matmul(x, w_noise)\n",
    "      noise_stddev = ((tf.nn.softplus(raw_noise_stddev) + noise_epsilon) *\n",
    "                      (tf.to_float(train)))\n",
    "      noisy_logits = clean_logits + (\n",
    "          tf.random_normal(tf.shape(clean_logits)) * noise_stddev)\n",
    "      logits = noisy_logits\n",
    "      if common_layers.should_generate_summaries():\n",
    "        tf.summary.histogram(\"noisy_logits\", noisy_logits)\n",
    "        tf.summary.histogram(\"noise_stddev\", noise_stddev)\n",
    "    else:\n",
    "      logits = clean_logits\n",
    "    top_logits, top_indices = _my_top_k(logits, min(k + 1, num_experts))\n",
    "    # top k logits has shape [batch, k]\n",
    "    top_k_logits = tf.slice(top_logits, [0, 0], [-1, k])\n",
    "    top_k_indices = tf.slice(top_indices, [0, 0], [-1, k])\n",
    "    top_k_gates = tf.nn.softmax(top_k_logits)\n",
    "    # This will be a `Tensor` of shape `[batch_size, n]`, with zeros in the\n",
    "    # positions corresponding to all but the top k experts per example.\n",
    "    gates = _rowwise_unsorted_segment_sum(top_k_gates, top_k_indices,\n",
    "                                          num_experts)\n",
    "    if noisy_gating and k < num_experts:\n",
    "      load = tf.reduce_sum(\n",
    "          _prob_in_top_k(clean_logits, noisy_logits, noise_stddev, top_logits,\n",
    "                         k), 0)\n",
    "    else:\n",
    "      load = _gates_to_load(gates)\n",
    "    if common_layers.should_generate_summaries():\n",
    "      tf.summary.histogram(\"importance\", tf.reduce_sum(gates, 0))\n",
    "      tf.summary.histogram(\"load\", load)\n",
    "    return gates, load\n",
    "\n",
    "\n",
    "class PadRemover(object):\n",
    "  \"\"\"Helper to remove padding from a tensor before sending to the experts.\n",
    "  The padding is computed for one reference tensor containing the padding mask\n",
    "  and then can be applied to any other tensor of shape [dim_origin,...].\n",
    "  Ex:\n",
    "      input = [\n",
    "        [tok1, tok2],\n",
    "        [tok3, tok4],\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [tok5, tok6],\n",
    "        [0, 0],\n",
    "      ]\n",
    "      output = [\n",
    "        [tok1, tok2],\n",
    "        [tok3, tok4],\n",
    "        [tok5, tok6],\n",
    "      ]\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, pad_mask):\n",
    "    \"\"\"Compute and store the location of the padding.\n",
    "    Args:\n",
    "      pad_mask (tf.Tensor): Reference padding tensor of shape\n",
    "        [batch_size,length] or [dim_origin] (dim_origin=batch_size*length)\n",
    "        containing non-zeros positive values to indicate padding location.\n",
    "    \"\"\"\n",
    "    self.nonpad_ids = None\n",
    "    self.dim_origin = None\n",
    "\n",
    "    with tf.name_scope(\"pad_reduce/get_ids\"):\n",
    "      pad_mask = tf.reshape(pad_mask, [-1])  # Flatten the batch\n",
    "      # nonpad_ids contains coordinates of zeros rows (as pad_mask is\n",
    "      # float32, checking zero equality is done with |x| < epsilon, with\n",
    "      # epsilon=1e-9 as standard, here pad_mask only contains positive values\n",
    "      # so tf.abs would be redundant)\n",
    "      self.nonpad_ids = tf.to_int32(tf.where(pad_mask < 1e-9))\n",
    "      self.dim_origin = tf.shape(pad_mask)[:1]\n",
    "\n",
    "  def remove(self, x):\n",
    "    \"\"\"Remove padding from the given tensor.\n",
    "    Args:\n",
    "      x (tf.Tensor): of shape [dim_origin,...]\n",
    "    Returns:\n",
    "      a tensor of shape [dim_compressed,...] with dim_compressed <= dim_origin\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"pad_reduce/remove\"):\n",
    "      x_shape = x.get_shape().as_list()\n",
    "      x = tf.gather_nd(\n",
    "          x,\n",
    "          indices=self.nonpad_ids,\n",
    "      )\n",
    "      if not tf.executing_eagerly():\n",
    "        # This is a hack but for some reason, gather_nd return a tensor of\n",
    "        # undefined shape, so the shape is set up manually\n",
    "        x.set_shape([None] + x_shape[1:])\n",
    "    return x\n",
    "\n",
    "  def restore(self, x):\n",
    "    \"\"\"Add padding back to the given tensor.\n",
    "    Args:\n",
    "      x (tf.Tensor): of shape [dim_compressed,...]\n",
    "    Returns:\n",
    "      a tensor of shape [dim_origin,...] with dim_compressed >= dim_origin. The\n",
    "      dim is restored from the original reference tensor\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"pad_reduce/restore\"):\n",
    "      x = tf.scatter_nd(\n",
    "          indices=self.nonpad_ids,\n",
    "          updates=x,\n",
    "          shape=tf.concat([self.dim_origin, tf.shape(x)[1:]], axis=0),\n",
    "      )\n",
    "    return x\n",
    "\n",
    "\n",
    "@add_name_scope(\"map_ids\")\n",
    "def map_ids(x, indices, map_fn):\n",
    "  \"\"\"Apply a function to each coordinate ids of a multidimensional tensor.\n",
    "  This allows to process each sequence of a batch independently. This is\n",
    "  similar to tf.map_fn but with tensor where the batch dim has been flatten.\n",
    "  Warning: The indices ids have to be contiguous and ordered in memory as the\n",
    "  output vector for each of the ids are simply concatenated after being\n",
    "  processed.\n",
    "  Ex: if your indices are [0,2,2,1,2,0], the output will contains the processed\n",
    "  rows in the following order: [0,0,1,2,2,2]\n",
    "  Args:\n",
    "    x (Tensor): The tensor to be dispatched of shape [length,...]\n",
    "    indices (Tensor): A int32 tensor of size [length, 1] containing the batch\n",
    "      coordinate of x\n",
    "    map_fn (fct): Function called for every ids of the original tensor. Take\n",
    "      as input a tensor of same rank than x and from shape [length_id,...] with\n",
    "      length_id <= length. Isn't called if length_id == 0\n",
    "  Returns:\n",
    "    a tensor of same shape as x, where each elements has been processed\n",
    "  \"\"\"\n",
    "  indices = tf.reshape(indices, [-1])\n",
    "\n",
    "  t_i = tf.constant(0)\n",
    "  # batch_coordinates start at 0\n",
    "  t_batch_size = tf.reduce_max(indices) + 1\n",
    "\n",
    "  # ta_stack_out will store the intermediate results for each individual id\n",
    "  # As alternative to tf.TensorArray, scatter_update could potentially be used\n",
    "  # but that would require an additional mutable tensor.\n",
    "  ta_stack_out = tf.TensorArray(\n",
    "      x.dtype,\n",
    "      size=t_batch_size,\n",
    "  )\n",
    "\n",
    "  # Then we iterate over each sequence individually and compute the\n",
    "  # transformation for each id\n",
    "  while_condition = lambda t_i, *args: tf.less(t_i, t_batch_size)\n",
    "  def body(t_i, ta_stack_out):\n",
    "    \"\"\"Loop body.\"\"\"\n",
    "    # Gather the ids\n",
    "    current_ids = tf.to_int32(tf.where(tf.equal(indices, t_i)))\n",
    "    t_row = tf.gather_nd(x, indices=current_ids)\n",
    "\n",
    "    # TODO(epot): Should not call map_fn if t_row size is 0\n",
    "\n",
    "    # Apply transformation to each id\n",
    "    # Restore batch_dim=1 as most function expect [batch_dim, length, ...] as\n",
    "    # input\n",
    "    t_row = tf.expand_dims(t_row, axis=0)\n",
    "    t_row = map_fn(t_row)\n",
    "    t_row = tf.squeeze(t_row, axis=0)  # Squeeze for concatenation\n",
    "    ta_stack_out = ta_stack_out.write(t_i, t_row)\n",
    "\n",
    "    return [tf.add(t_i, 1), ta_stack_out]  # ++i\n",
    "\n",
    "  # Run the loop, equivalent to:\n",
    "  # stack_out = []\n",
    "  # while i < batch_size:\n",
    "  #   stack_out.expand(map_fn(x[indices==i]))\n",
    "  _, ta_stack_out = tf.while_loop(while_condition, body, [t_i, ta_stack_out])\n",
    "\n",
    "  # Merge all results\n",
    "  return ta_stack_out.concat()\n",
    "\n",
    "\n",
    "def transpose_list_of_lists(lol):\n",
    "  \"\"\"Transpose a list of equally-sized python lists.\n",
    "  Args:\n",
    "    lol: a list of lists\n",
    "  Returns:\n",
    "    a list of lists\n",
    "  \"\"\"\n",
    "  assert lol, \"cannot pass the empty list\"\n",
    "  return [list(x) for x in zip(*lol)]\n",
    "\n",
    "\n",
    "\n",
    "def flatten_all_but_last(a):\n",
    "  \"\"\"Flatten all dimensions of a except the last.\"\"\"\n",
    "  ret = tf.reshape(a, [-1, tf.shape(a)[-1]])\n",
    "  if not tf.executing_eagerly():\n",
    "    ret.set_shape([None] + a.get_shape().as_list()[-1:])\n",
    "  return ret\n",
    "\n",
    "\n",
    "def reduce_by_device(parallelism, data, reduce_fn):\n",
    "  \"\"\"Reduces data per device.\n",
    "  This can be useful, for example, if we want to all-reduce n tensors on k<n\n",
    "  devices (like during eval when we have only one device).  We call\n",
    "  reduce_by_device() to first sum the tensors per device, then call our usual\n",
    "  all-reduce operation to create one sum per device, followed by\n",
    "  expand_by_device, to create the appropriate number of pointers to these\n",
    "  results.  See all_reduce_ring() below for an example of how this is used.\n",
    "  Args:\n",
    "    parallelism: a expert_utils.Parallelism object\n",
    "    data: a list of Tensors with length parallelism.n\n",
    "    reduce_fn: a function taking a list of Tensors.  e.g. tf.add_n\n",
    "  Returns:\n",
    "    device_parallelism: a Parallelism object with each device listed only once.\n",
    "    reduced_data: A list of Tensors, one per device.\n",
    "  \"\"\"\n",
    "  unique_devices = []\n",
    "  device_to_data = {}\n",
    "  for dev, datum in zip(parallelism.devices, data):\n",
    "    if dev not in device_to_data:\n",
    "      unique_devices.append(dev)\n",
    "      device_to_data[dev] = [datum]\n",
    "    else:\n",
    "      device_to_data[dev].append(datum)\n",
    "  device_parallelism = Parallelism(unique_devices)\n",
    "  grouped_data = [device_to_data[dev] for dev in unique_devices]\n",
    "  return device_parallelism, device_parallelism(reduce_fn, grouped_data)\n",
    "\n",
    "\n",
    "def expand_by_device(original_parallelism, device_parallelism, data):\n",
    "  \"\"\"Opposite of reduce_by_device().\n",
    "  Args:\n",
    "    original_parallelism: a expert_utils.Parallelism object.\n",
    "    device_parallelism: a expert_utils.Parallelism object.\n",
    "    data: a list of tensors with length device_parallelism.n\n",
    "  Returns:\n",
    "    a list of Tensors with length original_parallelism.n\n",
    "  \"\"\"\n",
    "  device_to_datum = {\n",
    "      device_parallelism.devices[i]: data[i]\n",
    "      for i in range(device_parallelism.n)}\n",
    "  return [device_to_datum[d] for d in original_parallelism.devices]\n",
    "\n",
    "\n",
    "def all_reduce_ring(x, parallelism, maybe_reduce=True, use_bfloat16=True):\n",
    "  \"\"\"Compute the sum of all Tensors and put the result everywhere.\n",
    "  Assumes that the devices are connected in a ring.\n",
    "  Args:\n",
    "    x: a list of Tensors with length parallelism.n\n",
    "    parallelism: a expert_utils.Parallelism object.\n",
    "    maybe_reduce: a boolean - first reduce per device.\n",
    "    use_bfloat16: a boolean - saves bandwidth but loses precision\n",
    "  Returns:\n",
    "    a list of Tensors with length parallelism.n\n",
    "  \"\"\"\n",
    "  if parallelism.n == 1:\n",
    "    return x\n",
    "\n",
    "  if maybe_reduce:\n",
    "    original_parallelism = parallelism\n",
    "    parallelism, x = reduce_by_device(parallelism, x, tf.add_n)\n",
    "\n",
    "  if parallelism.n == 1:\n",
    "    y = x\n",
    "  else:\n",
    "    # first shard the input:\n",
    "    x_flat = parallelism(tf.reshape, x, [[-1]] * parallelism.n)\n",
    "    # [device, shard]\n",
    "    x_split = parallelism(\n",
    "        common_layers.approximate_split, x_flat, parallelism.n, 0)\n",
    "    def _step(source_replica, target_replica, x_split, op=\"plus_eq\"):\n",
    "      \"\"\"Helper function - one step of summing or copying.\n",
    "      If op == \"plus_eq\", then adds source_replica into target_replica\n",
    "      If op == \"copy\", then copies source_replica onto target_replica\n",
    "      These operations happen for all shards.  The replica numbers are offset\n",
    "      by the shard numbers to keep all physical links busy.\n",
    "      Args:\n",
    "        source_replica: an integer\n",
    "        target_replica: an integer\n",
    "        x_split: a list of lists of tensors\n",
    "        op: a string\n",
    "      \"\"\"\n",
    "      for shard in range(parallelism.n):\n",
    "        source_device = (shard + source_replica) % parallelism.n\n",
    "        target_device = (shard + target_replica) % parallelism.n\n",
    "        source = x_split[source_device][shard]\n",
    "        if use_bfloat16:\n",
    "          with tf.device(parallelism.devices[source_device]):\n",
    "            source = tf.to_bfloat16(source)\n",
    "        with tf.device(parallelism.devices[target_device]):\n",
    "          source = tf.to_float(source)\n",
    "          if op == \"plus_eq\":\n",
    "            x_split[target_device][shard] += source\n",
    "          else:\n",
    "            assert op == \"copy\"\n",
    "            x_split[target_device][shard] = tf.identity(source)\n",
    "    center = parallelism.n // 2\n",
    "\n",
    "    # accumulate everything towards the center.\n",
    "    for i in reversed(range(center, parallelism.n - 1)):\n",
    "      _step(i + 1, i, x_split, op=\"plus_eq\")\n",
    "    for i in range(center):\n",
    "      _step(i, i + 1, x_split, op=\"plus_eq\")\n",
    "    # copy everything away from the center.\n",
    "    for i in range(center, parallelism.n - 1):\n",
    "      _step(i, i + 1, x_split, op=\"copy\")\n",
    "    for i in reversed(range(center)):\n",
    "      _step(i + 1, i, x_split, op=\"copy\")\n",
    "    x_concat = parallelism(tf.concat, x_split, 0)\n",
    "    y = parallelism(common_layers.reshape_like_all_dims, x_concat, x)\n",
    "  if maybe_reduce:\n",
    "    y = expand_by_device(original_parallelism, parallelism, y)\n",
    "  return y\n",
    "\n",
    "\n",
    "class SparseDispatcher(object):\n",
    "  \"\"\"Helper for implementing a mixture of experts.\n",
    "  The purpose of this class is to create input minibatches for the\n",
    "  experts and to combine the results of the experts to form a unified\n",
    "  output tensor.\n",
    "  There are two functions:\n",
    "    dispatch - take an input Tensor and create input Tensors for each expert.\n",
    "    combine - take output Tensors from each expert and form a combined output\n",
    "      Tensor.  Outputs from different experts for the same batch element are\n",
    "      summed together, weighted by the provided \"gates\".\n",
    "  The class is initialized with a \"gates\" Tensor, which specifies which\n",
    "  batch elements go to which experts, and the weights to use when combining\n",
    "  the outputs.  Batch element b is sent to expert e iff gates[b, e] != 0.\n",
    "  The inputs and outputs are all two-dimensional [batch, depth].\n",
    "  Caller is responsible for collapsing additional dimensions prior to\n",
    "  calling this class and reshaping the output to the original shape.\n",
    "  See common_layers.reshape_like().\n",
    "  Example use:\n",
    "  gates: a float32 `Tensor` with shape `[batch_size, num_experts]`\n",
    "  inputs: a float32 `Tensor` with shape `[batch_size, input_size]`\n",
    "  experts: a list of length `num_experts` containing sub-networks.\n",
    "    dispatcher = SparseDispatcher(num_experts, gates)\n",
    "    expert_inputs = dispatcher.dispatch(inputs)\n",
    "    expert_outputs = [experts[i](expert_inputs[i]) for i in range(num_experts)]\n",
    "    outputs = dispatcher.combine(expert_outputs)\n",
    "  The preceding code sets the output for a particular example b to:\n",
    "  output[b] = Sum_i(gates[b, i] * experts[i](inputs[b]))\n",
    "  This class takes advantage of sparsity in the gate matrix by including in the\n",
    "  `Tensor`s for expert i only the batch elements for which `gates[b, i] > 0`.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, num_experts, gates):\n",
    "    \"\"\"Create a SparseDispatcher.\n",
    "    Args:\n",
    "      num_experts: an integer.\n",
    "      gates: a `Tensor` of shape `[batch_size, num_experts]`.\n",
    "    Returns:\n",
    "      a SparseDispatcher\n",
    "    \"\"\"\n",
    "    self._gates = gates\n",
    "    self._num_experts = num_experts\n",
    "\n",
    "    where = tf.to_int32(tf.where(tf.transpose(gates) > 0))\n",
    "    self._expert_index, self._batch_index = tf.unstack(where, num=2, axis=1)\n",
    "    self._part_sizes_tensor = tf.reduce_sum(tf.to_int32(gates > 0), [0])\n",
    "    self._nonzero_gates = tf.gather(\n",
    "        tf.reshape(self._gates, [-1]),\n",
    "        self._batch_index * num_experts + self._expert_index)\n",
    "\n",
    "  @add_name_scope()\n",
    "  def dispatch(self, inp):\n",
    "    \"\"\"Create one input Tensor for each expert.\n",
    "    The `Tensor` for a expert `i` contains the slices of `inp` corresponding\n",
    "    to the batch elements `b` where `gates[b, i] > 0`.\n",
    "    Args:\n",
    "      inp: a `Tensor` of shape \"[batch_size, <extra_input_dims>]`\n",
    "    Returns:\n",
    "      a list of `num_experts` `Tensor`s with shapes\n",
    "        `[expert_batch_size_i, <extra_input_dims>]`.\n",
    "    \"\"\"\n",
    "    inp = tf.gather(inp, self._batch_index)\n",
    "    return tf.split(inp, self._part_sizes_tensor, 0, num=self._num_experts)\n",
    "\n",
    "  @add_name_scope()\n",
    "  def combine(self, expert_out, multiply_by_gates=True):\n",
    "    \"\"\"Sum together the expert output, weighted by the gates.\n",
    "    The slice corresponding to a particular batch element `b` is computed\n",
    "    as the sum over all experts `i` of the expert output, weighted by the\n",
    "    corresponding gate values.  If `multiply_by_gates` is set to False, the\n",
    "    gate values are ignored.\n",
    "    Args:\n",
    "      expert_out: a list of `num_experts` `Tensor`s, each with shape\n",
    "        `[expert_batch_size_i, <extra_output_dims>]`.\n",
    "      multiply_by_gates: a boolean\n",
    "    Returns:\n",
    "      a `Tensor` with shape `[batch_size, <extra_output_dims>]`.\n",
    "    \"\"\"\n",
    "    # see comments on convert_gradient_to_tensor\n",
    "    stitched = common_layers.convert_gradient_to_tensor(\n",
    "        tf.concat(expert_out, 0))\n",
    "    if multiply_by_gates:\n",
    "      stitched *= tf.expand_dims(self._nonzero_gates, 1)\n",
    "    combined = tf.unsorted_segment_sum(stitched, self._batch_index,\n",
    "                                       tf.shape(self._gates)[0])\n",
    "    return combined\n",
    "\n",
    "  def expert_to_gates(self):\n",
    "    \"\"\"Gate values corresponding to the examples in the per-expert `Tensor`s.\n",
    "    Returns:\n",
    "      a list of `num_experts` one-dimensional `Tensor`s with type `tf.float32`\n",
    "          and shapes `[expert_batch_size_i]`\n",
    "    \"\"\"\n",
    "    return tf.split(\n",
    "        self._nonzero_gates, self._part_sizes_tensor, 0, num=self._num_experts)\n",
    "\n",
    "  def expert_to_batch_indices(self):\n",
    "    \"\"\"Batch indices corresponding to the examples in the per-expert `Tensor`s.\n",
    "    Returns:\n",
    "      a list of `num_experts` one-dimensional `Tensor`s with type `tf.int64`\n",
    "          and shapes `[expert_batch_size_i]`\n",
    "    \"\"\"\n",
    "    return tf.split(\n",
    "        self._batch_index, self._part_sizes_tensor, 0, num=self._num_experts)\n",
    "\n",
    "  @property\n",
    "  def part_sizes(self):\n",
    "    return self._part_sizes_tensor\n",
    "\n",
    "\n",
    "\n",
    "def local_moe(x,\n",
    "              train,\n",
    "              expert_fn,\n",
    "              num_experts,\n",
    "              k=1,\n",
    "              loss_coef=1e-2,\n",
    "              hparams=None,\n",
    "              pass_x=True,\n",
    "              pass_gates=False,\n",
    "              additional_dispatch_params=None,\n",
    "              name=None):\n",
    "  \"\"\"Call a local mixture of experts.\n",
    "  Args:\n",
    "    x: a tensors with shape [... , input_size]\n",
    "    train: a boolean scalar.\n",
    "    expert_fn: a function.\n",
    "    num_experts: an integer - number of experts\n",
    "    k: an integer - how many experts to use for each batch element\n",
    "    loss_coef: a scalar - multiplier on load-balancing losses\n",
    "    hparams: optional hparams for vq gating\n",
    "    pass_x: a boolean. If true, x will also be dispatched to the experts.\n",
    "    pass_gates: a boolean. If true, gates will be passed to experts. Might be\n",
    "      necessary when dealing with sparse encoder-encoder decoder attention\n",
    "    additional_dispatch_params: The extra tensors that need to be sent to each\n",
    "      expert. Examples include batch batch coordinates (see\n",
    "      common_attention.local_expert_attention)\n",
    "    name: a string\n",
    "  Returns:\n",
    "    y: a tensor.  Has the same shape as x, except for the last dimension,\n",
    "      which is output_size.\n",
    "    extra_training_loss: a scalar.  This should be added into the overall\n",
    "      training loss of the model.  The backpropagation of this loss\n",
    "      encourages all experts to be approximately equally used across a batch.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(name, default_name=\"local_moe\"):\n",
    "    centroids = None\n",
    "    x_flat = flatten_all_but_last(x)\n",
    "    if True:\n",
    "      tf.logging.info(\"Using noisy top_k with k = {}\".format(k))\n",
    "      # The gates indicate which batch elements go to which tensors.\n",
    "      # load is a measure of approximately how many examples go to each expert\n",
    "      gates, load = noisy_top_k_gating(\n",
    "          x_flat,\n",
    "          num_experts,\n",
    "          train,\n",
    "          k,\n",
    "          initializer=tf.zeros_initializer(),\n",
    "          noisy_gating=True,\n",
    "          noise_epsilon=1e-2)\n",
    "      importance = tf.reduce_sum(gates, 0)\n",
    "      loss = (cv_squared(importance) + cv_squared(load))\n",
    "    loss *= loss_coef\n",
    "    # Shuffle data between datashards and experts.\n",
    "    dispatcher = SparseDispatcher(num_experts, gates)\n",
    "    # Set up expert_fn arguments\n",
    "    expert_kwargs = {}\n",
    "    if pass_x:\n",
    "      expert_kwargs[\"x\"] = dispatcher.dispatch(x_flat)\n",
    "    if pass_gates:\n",
    "      expert_kwargs[\"gates\"] = dispatcher.expert_to_gates()\n",
    "    for key, val in six.iteritems(additional_dispatch_params or {}):\n",
    "      val = flatten_all_but_last(val)\n",
    "      expert_kwargs[key] = dispatcher.dispatch(val)\n",
    "\n",
    "    ep = Parallelism([DEFAULT_DEV_STRING] * num_experts, reuse=None)\n",
    "    expert_outputs = ep(expert_fn, **expert_kwargs)\n",
    "\n",
    "    y_flat = dispatcher.combine(expert_outputs)\n",
    "    if centroids is not None:\n",
    "      centroids = tf.squeeze(centroids, axis=[1, 2])\n",
    "      y_flat += centroids\n",
    "    y = common_layers.reshape_like(y_flat, x)\n",
    "    return y, loss\n",
    "\n",
    "def ffn_expert_fn(input_size,\n",
    "                  hidden_sizes,\n",
    "                  output_size,\n",
    "                  hidden_activation=tf.nn.relu):\n",
    "  \"\"\"Returns a function that creates a feed-forward network.\n",
    "  Use this function to create the expert_fn argument to distributed_moe.\n",
    "  Args:\n",
    "    input_size: an integer\n",
    "    hidden_sizes: a list of integers\n",
    "    output_size: an integer\n",
    "    hidden_activation: a unary function.\n",
    "  Returns:\n",
    "    a unary function\n",
    "  \"\"\"\n",
    "  def my_fn(x):\n",
    "    layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "    for i in range(1 + len(hidden_sizes)):\n",
    "      w = tf.Variable(tf.random.truncated_normal(layer_sizes[i:i+2], stddev=0.01))\n",
    "      x = tf.matmul(x, w)\n",
    "      if i < len(hidden_sizes):\n",
    "        x = hidden_activation(x)\n",
    "      if layer_sizes[i] != input_size:\n",
    "        x *= (layer_sizes[i] / float(input_size))**-0.5\n",
    "    return x\n",
    "  return my_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-cfa4808b6932>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.Session()\n",
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-2735aa7c9291>:496: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-2-2735aa7c9291>:383: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From <ipython-input-2-2735aa7c9291>:246: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From <ipython-input-2-2735aa7c9291>:843: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "inputs = tf.compat.v1.placeholder(tf.float32, [None, 784])\n",
    "targets = tf.compat.v1.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "num_experts = 2\n",
    "experts = [ffn_expert_fn(784, [512], 10) for j in range(num_experts)]\n",
    "\n",
    "gates, load = noisy_top_k_gating(\n",
    "      inputs,\n",
    "      num_experts,\n",
    "      True,\n",
    "      num_experts,\n",
    "      initializer=tf.zeros_initializer(),\n",
    "      noisy_gating=True,\n",
    "      noise_epsilon=1e-2)\n",
    "\n",
    "dispatcher = SparseDispatcher(num_experts, gates)\n",
    "expert_inputs = dispatcher.dispatch(inputs)\n",
    "\n",
    "\n",
    "expert_outputs = []\n",
    "for _ in range(num_experts):\n",
    "    expert_outputs.append(tf.compat.v1.placeholder(tf.float32, [None, 10]))\n",
    "\n",
    "outputs = dispatcher.combine(expert_outputs)\n",
    "\n",
    "s.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(3, 784)\n",
      "[[0.20589589 0.79410416]\n",
      " [0.71042526 0.28957474]\n",
      " [0.6292774  0.37072256]]\n",
      "{<tf.Tensor 'Placeholder:0' shape=(?, 784) dtype=float32>: array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), <tf.Tensor 'Placeholder_1:0' shape=(?, 10) dtype=float32>: array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]), <tf.Tensor 'Placeholder_2:0' shape=(?, 10) dtype=float32>: array([[0.38992875, 0.32585877, 0.46602385, 0.88407744, 0.08328068,\n",
      "        0.01018556, 0.09388733, 0.03242398, 0.43953326, 0.33231509],\n",
      "       [0.24884418, 0.78571652, 0.88401591, 0.8808208 , 0.09598936,\n",
      "        0.48373881, 0.13135773, 0.99825809, 0.52238275, 0.5304318 ],\n",
      "       [0.37437846, 0.85499575, 0.23600805, 0.79124487, 0.70552445,\n",
      "        0.35543553, 0.73503858, 0.33051599, 0.48531366, 0.15962678]]), <tf.Tensor 'Placeholder_3:0' shape=(?, 10) dtype=float32>: array([[0.18482532, 0.40911562, 0.7187311 , 0.10709958, 0.07466758,\n",
      "        0.88035772, 0.39026414, 0.75743537, 0.01113427, 0.60452306],\n",
      "       [0.05233204, 0.20705039, 0.21375211, 0.86238856, 0.93616661,\n",
      "        0.04683979, 0.02612249, 0.67228888, 0.33199935, 0.04028435],\n",
      "       [0.64115704, 0.84242168, 0.89039643, 0.39906133, 0.97450731,\n",
      "        0.04157487, 0.05514597, 0.25316056, 0.43431753, 0.73242964]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 3\n",
    "batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "feeds = {\n",
    "    inputs: batch_x,\n",
    "    targets: batch_y,\n",
    "}\n",
    "exp_inp = s.run([expert_inputs, gates], feeds)\n",
    "\n",
    "print (len(exp_inp[0]))\n",
    "print (exp_inp[0][0].shape)\n",
    "print (exp_inp[1])\n",
    "\n",
    "feeds = {\n",
    "    inputs: batch_x,\n",
    "    targets: batch_y,\n",
    "}\n",
    "for i in range(num_experts):\n",
    "    feeds[expert_outputs[i]] = np.random.rand(batch_size,10)\n",
    "\n",
    "print (feeds)\n",
    "\n",
    "out = s.run(outputs, feeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
