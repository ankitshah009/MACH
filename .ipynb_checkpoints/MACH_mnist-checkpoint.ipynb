{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1011 08:20:52.946810 4539028928 __init__.py:690] \n",
      "\n",
      "  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.\n",
      "\n",
      "  Please upgrade your code to TensorFlow 2.0:\n",
      "    * https://www.tensorflow.org/beta/guide/migration_guide\n",
      "\n",
      "  Or install the latest stable TensorFlow 1.X release:\n",
      "    * `pip install -U \"tensorflow==1.*\"`\n",
      "\n",
      "  Otherwise your code may be broken by the change.\n",
      "\n",
      "  \n",
      "W1011 08:20:54.710767 4539028928 deprecation.py:323] From <ipython-input-3-e74d6586e2ad>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W1011 08:20:54.711786 4539028928 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W1011 08:20:54.712568 4539028928 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W1011 08:20:54.886940 4539028928 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W1011 08:20:54.890423 4539028928 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1011 08:20:54.930864 4539028928 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "import queue\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.n_input = 784  # input layer (28x28 pixels)\n",
    "        self.n_hidden1 = 512  # 1st hidden layer\n",
    "        self.n_hidden2 = 256  # 2nd hidden layer\n",
    "        self.n_embedding = 128  # 3rd embedding layer\n",
    "        self.n_output = 10  # output layer (0-9 digits)\n",
    "        self.n_model_inputs = self.n_input + self.n_embedding  # input layer (28x28 pixels)\n",
    "\n",
    "        self.learning_rate = 1e-4\n",
    "        self.learn_iterations = 1000\n",
    "        self.batch_size = 128\n",
    "        self.mach_batch_size = 512\n",
    "\n",
    "\n",
    "class Mach:\n",
    "\n",
    "    def __init__(self, config, mnist):\n",
    "        self.c = config\n",
    "        self.mnist = mnist\n",
    "        self.n_train = self.mnist.train.num_examples  # 55,000\n",
    "        self.n_validation = self.mnist.validation.num_examples  # 5000\n",
    "        self.n_test = self.mnist.test.num_examples  # 10,000\n",
    "        \n",
    "        self.gradient_queue = queue.LifoQueue(maxsize=-1)\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "        self.session = tf.Session(graph=self.graph)\n",
    "        with self.graph.as_default():\n",
    "            self.input_fn()\n",
    "            self.model_fn()\n",
    "            self.grad_fn()\n",
    "            init = tf.global_variables_initializer()\n",
    "        self.session.run(init)\n",
    "        \n",
    "        self.child = None\n",
    "        \n",
    "    def input_fn(self):\n",
    "        # Placeholders.\n",
    "        self.X = tf.placeholder(tf.float32, [None, self.c.n_input], 'X')\n",
    "        self.Y = tf.placeholder(tf.float32, [None, self.c.n_output], 'Y')\n",
    "        self.C = tf.placeholder(tf.float32, [None, self.c.n_embedding], 'C')\n",
    "            \n",
    "    def grad_fn(self):        \n",
    "        # Optimizer.\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.c.learning_rate)\n",
    "        \n",
    "        # Gradients.\n",
    "        self.E_grad = tf.placeholder(tf.float32, [None, self.c.n_embedding], 'E')\n",
    "        self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.Y, logits=self.logits))\n",
    "        self.upstream_gradients = self.optimizer.compute_gradients(loss=self.cross_entropy)\n",
    "        self.local_gradients = self.optimizer.compute_gradients(loss=self.cross_entropy)\n",
    "        \n",
    "        # Secondary metrics.\n",
    "        correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        # Upstream Gradient placeholders.\n",
    "        self.upstream_gradient_values = []\n",
    "        self.upstream_placeholder_gradients = []\n",
    "        for gradient_variable in self.upstream_gradients:\n",
    "            grad_placeholder = tf.placeholder(tf.float32, shape=gradient_variable[1].get_shape())\n",
    "            self.upstream_gradient_values.append(gradient_variable[1])\n",
    "            self.upstream_placeholder_gradients.append((grad_placeholder, gradient_variable[1]))\n",
    "            \n",
    "        # Local Gradient placeholders.\n",
    "        self.local_gradient_values = []\n",
    "        self.local_placeholder_gradients = []\n",
    "        for gradient_variable in self.local_gradients:\n",
    "            grad_placeholder = tf.placeholder(tf.float32, shape=gradient_variable[1].get_shape())\n",
    "            self.local_gradient_values.append(gradient_variable[1])\n",
    "            self.local_placeholder_gradients.append((grad_placeholder, gradient_variable[1]))\n",
    "\n",
    "        # Train steps.\n",
    "        self.upstream_train_step = self.optimizer.apply_gradients(self.upstream_placeholder_gradients)\n",
    "        self.local_train_step = self.optimizer.apply_gradients(self.local_placeholder_gradients)\n",
    "    \n",
    "    \n",
    "    def model_fn(self):\n",
    "        self.model_inputs = tf.concat([self.X, self.C], axis=1)\n",
    "        \n",
    "        weights = {\n",
    "            'w1': tf.Variable(tf.truncated_normal([self.c.n_model_inputs, self.c.n_hidden1], stddev=0.1)),\n",
    "            'w2': tf.Variable(tf.truncated_normal([self.c.n_hidden1, self.c.n_hidden2], stddev=0.1)),\n",
    "            'w3': tf.Variable(tf.truncated_normal([self.c.n_hidden2, self.c.n_embedding], stddev=0.1)),\n",
    "            'out': tf.Variable(tf.truncated_normal([self.c.n_embedding, self.c.n_output], stddev=0.1)),\n",
    "        }\n",
    "\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.constant(0.1, shape=[self.c.n_hidden1])),\n",
    "            'b2': tf.Variable(tf.constant(0.1, shape=[self.c.n_hidden2])),\n",
    "            'b3': tf.Variable(tf.constant(0.1, shape=[self.c.n_embedding])),\n",
    "            'out': tf.Variable(tf.constant(0.1, shape=[self.c.n_output]))\n",
    "        }\n",
    "\n",
    "        # Model feature extraction.\n",
    "        self.layer_1 = tf.add(tf.matmul(self.model_inputs, weights['w1']), biases['b1'])\n",
    "        self.layer_2 = tf.add(tf.matmul(self.layer_1, weights['w2']), biases['b2'])\n",
    "        self.E = tf.add(tf.matmul(self.layer_2, weights['w3']), biases['b3'])\n",
    "        \n",
    "        # Local logits.\n",
    "        self.logits = tf.matmul(self.E, weights['out']) + biases['out']\n",
    "        \n",
    "    def Child(self, batch):\n",
    "        if self.child == None:\n",
    "            return numpy.zeros((numpy.shape(batch)[0], self.c.n_embedding))\n",
    "        else:\n",
    "            return self.child.Spike(batch)\n",
    "        \n",
    "    def Spike(self, spikes):\n",
    "        feeds={self.X: spikes, self.C: self.Child(spikes), self.keep_prob: 1.0}\n",
    "        return self.session.run([self.E], feed_dict=feeds)[0]\n",
    "    \n",
    "    def Grade(self, grads, spikes):\n",
    "        cspikes = self.Child(spikes)\n",
    "        feeds={\n",
    "                self.X: batch, \n",
    "                self.C: self.Child(batch),\n",
    "                self.E_grad: grads\n",
    "        }\n",
    "        fetches = [self.upstream_gradient_values]\n",
    "        upstream_gradients = self.session.run(fetches, feeds)[0]\n",
    "        self.grad_queue.put(upstream_gradients)\n",
    "        \n",
    "    def Perf(self):\n",
    "        batch_x, batch_y = self.mnist.train.next_batch(self.c.batch_size)\n",
    "        fetches = [self.cross_entropy, self.accuracy]\n",
    "        feeds = {self.X: batch_x, self.Y: batch_y, self.C: self.Child(batch_x)}\n",
    "        loss, accuracy = self.session.run(fetches,feeds)    \n",
    "        print(\"Loss =\", str(loss), \"\\t| Accuracy =\", str(accuracy))\n",
    "\n",
    "    def Train(self, n):\n",
    "        for i in range(n):\n",
    "            self.batch_step()\n",
    "\n",
    "    def batch_step(self):\n",
    "        batch_x, batch_y = self.mnist.train.next_batch(self.c.batch_size)\n",
    "        feeds={self.X: batch_x, self.Y: batch_y, self.C: self.Child(batch_x)}\n",
    "        fetches=self.local_gradients\n",
    "        gradients = self.session.run(self.local_gradients, feed_dict=feeds)\n",
    "        self.gradient_queue.put(gradients)\n",
    "\n",
    "        fetches = [self.cross_entropy, self.accuracy]\n",
    "        feeds = {self.X: batch_x, self.Y: batch_y, self.C: self.Child(batch_x)}\n",
    "        minibatch_loss, minibatch_accuracy = self.session.run(fetches,feeds)    \n",
    "        print(\n",
    "            \"Iteration\",\n",
    "            str(i),\n",
    "            \"\\t| Loss =\",\n",
    "            str(minibatch_loss),\n",
    "            \"\\t| Accuracy =\",\n",
    "            str(minibatch_accuracy)\n",
    "            )\n",
    "\n",
    "    def Learn(self, n):\n",
    "        for i in range(n):\n",
    "            self.learn_step()\n",
    "        \n",
    "    def learn_step(self):\n",
    "        gradients = self.gradient_queue.get()\n",
    "        feeds = {}\n",
    "        for j, grad_var in enumerate(gradients):\n",
    "            feeds[self.local_placeholder_gradients[j][0]] = gradients[j][0]\n",
    "        self.session.run(self.local_train_step, feeds)\n",
    "    \n",
    "        \n",
    "#     def Train(self):\n",
    "#         # train on mini batches\n",
    "#         for i in range(self.c.learn_iterations):    \n",
    "            \n",
    "#             # Compute gradients.\n",
    "#             batch_x, batch_y = self.mnist.train.next_batch(self.c.batch_size)\n",
    "#             feeds={self.X: batch_x, self.Y: batch_y, self.C: self.Child(batch_x)}\n",
    "#             fetches=self.local_gradients\n",
    "#             gradients = self.session.run(self.local_gradients, feed_dict=feeds)\n",
    "            \n",
    "#             # Now apply them.\n",
    "#             # Feed batch of gradients.\n",
    "#             feeds = {}\n",
    "#             for j, grad_var in enumerate(gradients):\n",
    "#                 feeds[self.local_placeholder_gradients[j][0]] = gradients[j][0]\n",
    "\n",
    "#             # Apply gradients.\n",
    "#             self.session.run(self.local_train_step, feeds)\n",
    "                                     \n",
    "#             # print loss and accuracy (per minibatch)\n",
    "\n",
    "                \n",
    "    def Test(self):\n",
    "        feed_dict = {\n",
    "                self.X: self.mnist.test.images, \n",
    "                self.Y: self.mnist.test.labels, \n",
    "                self.C: self.Child(self.mnist.test.images) \n",
    "        }\n",
    "        test_accuracy = self.session.run(self.accuracy, feed_dict=feed_dict)\n",
    "        print(\"\\nAccuracy on test set:\", test_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 \t| Loss = 3.203428 \t| Accuracy = 0.140625\n",
      "Loss = 2.9019256 \t| Accuracy = 0.15625\n",
      "Iteration 1 \t| Loss = 2.99845 \t| Accuracy = 0.1171875\n",
      "Iteration 2 \t| Loss = 2.843295 \t| Accuracy = 0.15625\n",
      "Iteration 3 \t| Loss = 2.9312615 \t| Accuracy = 0.140625\n",
      "Iteration 4 \t| Loss = 2.4517303 \t| Accuracy = 0.203125\n",
      "Iteration 5 \t| Loss = 2.3996415 \t| Accuracy = 0.234375\n",
      "Iteration 6 \t| Loss = 2.2242515 \t| Accuracy = 0.265625\n",
      "Iteration 7 \t| Loss = 2.3805044 \t| Accuracy = 0.2421875\n",
      "Iteration 8 \t| Loss = 2.383777 \t| Accuracy = 0.265625\n",
      "Iteration 9 \t| Loss = 2.1066647 \t| Accuracy = 0.2890625\n",
      "Iteration 10 \t| Loss = 1.8743441 \t| Accuracy = 0.375\n",
      "Iteration 11 \t| Loss = 1.946922 \t| Accuracy = 0.3125\n",
      "Iteration 12 \t| Loss = 1.8661624 \t| Accuracy = 0.359375\n",
      "Iteration 13 \t| Loss = 1.5161598 \t| Accuracy = 0.5078125\n",
      "Iteration 14 \t| Loss = 1.5732443 \t| Accuracy = 0.4453125\n",
      "Iteration 15 \t| Loss = 1.3833759 \t| Accuracy = 0.4609375\n",
      "Iteration 16 \t| Loss = 1.569941 \t| Accuracy = 0.4453125\n",
      "Iteration 17 \t| Loss = 1.3629103 \t| Accuracy = 0.5390625\n",
      "Iteration 18 \t| Loss = 1.3682096 \t| Accuracy = 0.5234375\n",
      "Iteration 19 \t| Loss = 1.4920111 \t| Accuracy = 0.5078125\n",
      "Iteration 20 \t| Loss = 1.1758816 \t| Accuracy = 0.6640625\n",
      "Iteration 21 \t| Loss = 1.2261497 \t| Accuracy = 0.5859375\n",
      "Iteration 22 \t| Loss = 1.23718 \t| Accuracy = 0.640625\n",
      "Iteration 23 \t| Loss = 1.1604904 \t| Accuracy = 0.625\n",
      "Iteration 24 \t| Loss = 1.2104058 \t| Accuracy = 0.6328125\n",
      "Iteration 25 \t| Loss = 0.99658567 \t| Accuracy = 0.65625\n",
      "Iteration 26 \t| Loss = 1.0673301 \t| Accuracy = 0.640625\n",
      "Iteration 27 \t| Loss = 0.9933671 \t| Accuracy = 0.6640625\n",
      "Iteration 28 \t| Loss = 1.0879925 \t| Accuracy = 0.609375\n",
      "Iteration 29 \t| Loss = 0.97084415 \t| Accuracy = 0.6171875\n",
      "Iteration 30 \t| Loss = 0.9431827 \t| Accuracy = 0.671875\n",
      "Iteration 31 \t| Loss = 0.91339684 \t| Accuracy = 0.7109375\n",
      "Iteration 32 \t| Loss = 0.9088025 \t| Accuracy = 0.7265625\n",
      "Iteration 33 \t| Loss = 0.8682482 \t| Accuracy = 0.734375\n",
      "Iteration 34 \t| Loss = 0.79869413 \t| Accuracy = 0.7578125\n",
      "Iteration 35 \t| Loss = 0.8381264 \t| Accuracy = 0.71875\n",
      "Iteration 36 \t| Loss = 0.77662635 \t| Accuracy = 0.7421875\n",
      "Iteration 37 \t| Loss = 0.76037353 \t| Accuracy = 0.78125\n",
      "Iteration 38 \t| Loss = 0.73752177 \t| Accuracy = 0.7578125\n",
      "Iteration 39 \t| Loss = 0.61420244 \t| Accuracy = 0.8359375\n",
      "Iteration 40 \t| Loss = 0.92675686 \t| Accuracy = 0.7109375\n",
      "Iteration 41 \t| Loss = 0.89422417 \t| Accuracy = 0.6875\n",
      "Iteration 42 \t| Loss = 0.8662258 \t| Accuracy = 0.7265625\n",
      "Iteration 43 \t| Loss = 0.6738642 \t| Accuracy = 0.8046875\n",
      "Iteration 44 \t| Loss = 0.76661307 \t| Accuracy = 0.765625\n",
      "Iteration 45 \t| Loss = 0.81806576 \t| Accuracy = 0.78125\n",
      "Iteration 46 \t| Loss = 0.75306344 \t| Accuracy = 0.7421875\n",
      "Iteration 47 \t| Loss = 0.7205477 \t| Accuracy = 0.734375\n",
      "Iteration 48 \t| Loss = 0.54440105 \t| Accuracy = 0.8671875\n",
      "Iteration 49 \t| Loss = 0.74555516 \t| Accuracy = 0.7734375\n",
      "Iteration 50 \t| Loss = 0.63340235 \t| Accuracy = 0.8046875\n",
      "Iteration 51 \t| Loss = 0.5952792 \t| Accuracy = 0.8203125\n",
      "Iteration 52 \t| Loss = 0.65442896 \t| Accuracy = 0.8125\n",
      "Iteration 53 \t| Loss = 0.742589 \t| Accuracy = 0.765625\n",
      "Iteration 54 \t| Loss = 0.7993553 \t| Accuracy = 0.7421875\n",
      "Iteration 55 \t| Loss = 0.5977443 \t| Accuracy = 0.8046875\n",
      "Iteration 56 \t| Loss = 0.6198307 \t| Accuracy = 0.765625\n",
      "Iteration 57 \t| Loss = 0.5110384 \t| Accuracy = 0.8515625\n",
      "Iteration 58 \t| Loss = 0.6914812 \t| Accuracy = 0.7578125\n",
      "Iteration 59 \t| Loss = 0.78029454 \t| Accuracy = 0.7578125\n",
      "Iteration 60 \t| Loss = 0.6831421 \t| Accuracy = 0.8203125\n",
      "Iteration 61 \t| Loss = 0.63619596 \t| Accuracy = 0.796875\n",
      "Iteration 62 \t| Loss = 0.55388165 \t| Accuracy = 0.828125\n",
      "Iteration 63 \t| Loss = 0.65624666 \t| Accuracy = 0.8046875\n",
      "Iteration 64 \t| Loss = 0.46905762 \t| Accuracy = 0.8828125\n",
      "Iteration 65 \t| Loss = 0.48655578 \t| Accuracy = 0.890625\n",
      "Iteration 66 \t| Loss = 0.604965 \t| Accuracy = 0.8359375\n",
      "Iteration 67 \t| Loss = 0.5451063 \t| Accuracy = 0.8359375\n",
      "Iteration 68 \t| Loss = 0.68246424 \t| Accuracy = 0.78125\n",
      "Iteration 69 \t| Loss = 0.68687034 \t| Accuracy = 0.7890625\n",
      "Iteration 70 \t| Loss = 0.5497613 \t| Accuracy = 0.8515625\n",
      "Iteration 71 \t| Loss = 0.53657323 \t| Accuracy = 0.8203125\n",
      "Iteration 72 \t| Loss = 0.4202121 \t| Accuracy = 0.828125\n",
      "Iteration 73 \t| Loss = 0.5758804 \t| Accuracy = 0.8125\n",
      "Iteration 74 \t| Loss = 0.73572737 \t| Accuracy = 0.796875\n",
      "Iteration 75 \t| Loss = 0.5608604 \t| Accuracy = 0.796875\n",
      "Iteration 76 \t| Loss = 0.47407448 \t| Accuracy = 0.875\n",
      "Iteration 77 \t| Loss = 0.55707055 \t| Accuracy = 0.8046875\n",
      "Iteration 78 \t| Loss = 0.5332631 \t| Accuracy = 0.8125\n",
      "Iteration 79 \t| Loss = 0.49928018 \t| Accuracy = 0.8359375\n",
      "Iteration 80 \t| Loss = 0.5899551 \t| Accuracy = 0.828125\n",
      "Iteration 81 \t| Loss = 0.42799586 \t| Accuracy = 0.859375\n",
      "Iteration 82 \t| Loss = 0.46380246 \t| Accuracy = 0.8515625\n",
      "Iteration 83 \t| Loss = 0.50354975 \t| Accuracy = 0.84375\n",
      "Iteration 84 \t| Loss = 0.4931362 \t| Accuracy = 0.8515625\n",
      "Iteration 85 \t| Loss = 0.70580363 \t| Accuracy = 0.78125\n",
      "Iteration 86 \t| Loss = 0.48727685 \t| Accuracy = 0.875\n",
      "Iteration 87 \t| Loss = 0.7748889 \t| Accuracy = 0.7890625\n",
      "Iteration 88 \t| Loss = 0.6528046 \t| Accuracy = 0.7734375\n",
      "Iteration 89 \t| Loss = 0.70944846 \t| Accuracy = 0.8046875\n",
      "Iteration 90 \t| Loss = 0.52274597 \t| Accuracy = 0.8515625\n",
      "Iteration 91 \t| Loss = 0.48935738 \t| Accuracy = 0.84375\n",
      "Iteration 92 \t| Loss = 0.5785597 \t| Accuracy = 0.8359375\n",
      "Iteration 93 \t| Loss = 0.47136295 \t| Accuracy = 0.875\n",
      "Iteration 94 \t| Loss = 0.5053568 \t| Accuracy = 0.875\n",
      "Iteration 95 \t| Loss = 0.3242513 \t| Accuracy = 0.8984375\n",
      "Iteration 96 \t| Loss = 0.47404838 \t| Accuracy = 0.84375\n",
      "Iteration 97 \t| Loss = 0.48648325 \t| Accuracy = 0.84375\n",
      "Iteration 98 \t| Loss = 0.66607755 \t| Accuracy = 0.8359375\n",
      "Iteration 99 \t| Loss = 0.477758 \t| Accuracy = 0.8515625\n",
      "Iteration 100 \t| Loss = 0.5734863 \t| Accuracy = 0.8359375\n",
      "Loss = 0.48705077 \t| Accuracy = 0.8671875\n",
      "Iteration 101 \t| Loss = 0.56604546 \t| Accuracy = 0.8203125\n",
      "Iteration 102 \t| Loss = 0.32468313 \t| Accuracy = 0.90625\n",
      "Iteration 103 \t| Loss = 0.34352368 \t| Accuracy = 0.9296875\n",
      "Iteration 104 \t| Loss = 0.48111433 \t| Accuracy = 0.8671875\n",
      "Iteration 105 \t| Loss = 0.41508242 \t| Accuracy = 0.8671875\n",
      "Iteration 106 \t| Loss = 0.43651146 \t| Accuracy = 0.859375\n",
      "Iteration 107 \t| Loss = 0.4030853 \t| Accuracy = 0.875\n",
      "Iteration 108 \t| Loss = 0.36598885 \t| Accuracy = 0.890625\n",
      "Iteration 109 \t| Loss = 0.48825672 \t| Accuracy = 0.828125\n",
      "Iteration 110 \t| Loss = 0.35600823 \t| Accuracy = 0.890625\n",
      "Iteration 111 \t| Loss = 0.30284485 \t| Accuracy = 0.921875\n",
      "Iteration 112 \t| Loss = 0.3948211 \t| Accuracy = 0.8984375\n",
      "Iteration 113 \t| Loss = 0.4597512 \t| Accuracy = 0.859375\n",
      "Iteration 114 \t| Loss = 0.4455456 \t| Accuracy = 0.890625\n",
      "Iteration 115 \t| Loss = 0.40470427 \t| Accuracy = 0.859375\n",
      "Iteration 116 \t| Loss = 0.4554511 \t| Accuracy = 0.8515625\n",
      "Iteration 117 \t| Loss = 0.3574963 \t| Accuracy = 0.859375\n",
      "Iteration 118 \t| Loss = 0.54791725 \t| Accuracy = 0.828125\n",
      "Iteration 119 \t| Loss = 0.39361507 \t| Accuracy = 0.8671875\n",
      "Iteration 120 \t| Loss = 0.32795453 \t| Accuracy = 0.890625\n",
      "Iteration 121 \t| Loss = 0.4419742 \t| Accuracy = 0.84375\n",
      "Iteration 122 \t| Loss = 0.36806804 \t| Accuracy = 0.890625\n",
      "Iteration 123 \t| Loss = 0.5784894 \t| Accuracy = 0.8359375\n",
      "Iteration 124 \t| Loss = 0.3810103 \t| Accuracy = 0.90625\n",
      "Iteration 125 \t| Loss = 0.35949132 \t| Accuracy = 0.8984375\n",
      "Iteration 126 \t| Loss = 0.40844226 \t| Accuracy = 0.875\n",
      "Iteration 127 \t| Loss = 0.4491672 \t| Accuracy = 0.859375\n",
      "Iteration 128 \t| Loss = 0.45462647 \t| Accuracy = 0.828125\n",
      "Iteration 129 \t| Loss = 0.42328116 \t| Accuracy = 0.859375\n",
      "Iteration 130 \t| Loss = 0.4584527 \t| Accuracy = 0.875\n",
      "Iteration 131 \t| Loss = 0.39522135 \t| Accuracy = 0.890625\n",
      "Iteration 132 \t| Loss = 0.38689554 \t| Accuracy = 0.8828125\n",
      "Iteration 133 \t| Loss = 0.33978215 \t| Accuracy = 0.9375\n",
      "Iteration 134 \t| Loss = 0.35472792 \t| Accuracy = 0.890625\n",
      "Iteration 135 \t| Loss = 0.30877692 \t| Accuracy = 0.90625\n",
      "Iteration 136 \t| Loss = 0.53412604 \t| Accuracy = 0.875\n",
      "Iteration 137 \t| Loss = 0.6443322 \t| Accuracy = 0.828125\n",
      "Iteration 138 \t| Loss = 0.47113264 \t| Accuracy = 0.8671875\n",
      "Iteration 139 \t| Loss = 0.4570486 \t| Accuracy = 0.84375\n",
      "Iteration 140 \t| Loss = 0.48636726 \t| Accuracy = 0.859375\n",
      "Iteration 141 \t| Loss = 0.6015242 \t| Accuracy = 0.7734375\n",
      "Iteration 142 \t| Loss = 0.30049133 \t| Accuracy = 0.8984375\n",
      "Iteration 143 \t| Loss = 0.4765242 \t| Accuracy = 0.84375\n",
      "Iteration 144 \t| Loss = 0.40309936 \t| Accuracy = 0.8671875\n",
      "Iteration 145 \t| Loss = 0.4546822 \t| Accuracy = 0.875\n",
      "Iteration 146 \t| Loss = 0.38766047 \t| Accuracy = 0.875\n",
      "Iteration 147 \t| Loss = 0.3615928 \t| Accuracy = 0.875\n",
      "Iteration 148 \t| Loss = 0.3495451 \t| Accuracy = 0.8828125\n",
      "Iteration 149 \t| Loss = 0.4319202 \t| Accuracy = 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 150 \t| Loss = 0.39246753 \t| Accuracy = 0.8828125\n",
      "Iteration 151 \t| Loss = 0.3590059 \t| Accuracy = 0.890625\n",
      "Iteration 152 \t| Loss = 0.5109602 \t| Accuracy = 0.8515625\n",
      "Iteration 153 \t| Loss = 0.38340667 \t| Accuracy = 0.8671875\n",
      "Iteration 154 \t| Loss = 0.4706523 \t| Accuracy = 0.8671875\n",
      "Iteration 155 \t| Loss = 0.4898553 \t| Accuracy = 0.8515625\n",
      "Iteration 156 \t| Loss = 0.33808362 \t| Accuracy = 0.8828125\n",
      "Iteration 157 \t| Loss = 0.5145081 \t| Accuracy = 0.828125\n",
      "Iteration 158 \t| Loss = 0.3774789 \t| Accuracy = 0.875\n",
      "Iteration 159 \t| Loss = 0.39137152 \t| Accuracy = 0.875\n",
      "Iteration 160 \t| Loss = 0.36070585 \t| Accuracy = 0.90625\n",
      "Iteration 161 \t| Loss = 0.5079918 \t| Accuracy = 0.8359375\n",
      "Iteration 162 \t| Loss = 0.3530351 \t| Accuracy = 0.8984375\n",
      "Iteration 163 \t| Loss = 0.50493777 \t| Accuracy = 0.890625\n",
      "Iteration 164 \t| Loss = 0.41376936 \t| Accuracy = 0.8515625\n",
      "Iteration 165 \t| Loss = 0.57391095 \t| Accuracy = 0.8359375\n",
      "Iteration 166 \t| Loss = 0.4514685 \t| Accuracy = 0.859375\n",
      "Iteration 167 \t| Loss = 0.44189206 \t| Accuracy = 0.859375\n",
      "Iteration 168 \t| Loss = 0.38832578 \t| Accuracy = 0.8828125\n",
      "Iteration 169 \t| Loss = 0.47177112 \t| Accuracy = 0.8515625\n",
      "Iteration 170 \t| Loss = 0.42208287 \t| Accuracy = 0.8671875\n",
      "Iteration 171 \t| Loss = 0.27319247 \t| Accuracy = 0.9296875\n",
      "Iteration 172 \t| Loss = 0.36201438 \t| Accuracy = 0.890625\n",
      "Iteration 173 \t| Loss = 0.40368444 \t| Accuracy = 0.9140625\n",
      "Iteration 174 \t| Loss = 0.37549633 \t| Accuracy = 0.875\n",
      "Iteration 175 \t| Loss = 0.40708208 \t| Accuracy = 0.8984375\n",
      "Iteration 176 \t| Loss = 0.45817402 \t| Accuracy = 0.8515625\n",
      "Iteration 177 \t| Loss = 0.5063914 \t| Accuracy = 0.8671875\n",
      "Iteration 178 \t| Loss = 0.22272554 \t| Accuracy = 0.9375\n",
      "Iteration 179 \t| Loss = 0.45528492 \t| Accuracy = 0.859375\n",
      "Iteration 180 \t| Loss = 0.36931366 \t| Accuracy = 0.875\n",
      "Iteration 181 \t| Loss = 0.50067186 \t| Accuracy = 0.8359375\n",
      "Iteration 182 \t| Loss = 0.46095613 \t| Accuracy = 0.875\n",
      "Iteration 183 \t| Loss = 0.44316465 \t| Accuracy = 0.84375\n",
      "Iteration 184 \t| Loss = 0.44116113 \t| Accuracy = 0.8984375\n",
      "Iteration 185 \t| Loss = 0.37384585 \t| Accuracy = 0.890625\n",
      "Iteration 186 \t| Loss = 0.4035672 \t| Accuracy = 0.890625\n",
      "Iteration 187 \t| Loss = 0.3475633 \t| Accuracy = 0.8984375\n",
      "Iteration 188 \t| Loss = 0.49290246 \t| Accuracy = 0.8046875\n",
      "Iteration 189 \t| Loss = 0.4013642 \t| Accuracy = 0.90625\n",
      "Iteration 190 \t| Loss = 0.45074943 \t| Accuracy = 0.8828125\n",
      "Iteration 191 \t| Loss = 0.41961986 \t| Accuracy = 0.875\n",
      "Iteration 192 \t| Loss = 0.27791047 \t| Accuracy = 0.9375\n",
      "Iteration 193 \t| Loss = 0.3832242 \t| Accuracy = 0.8671875\n",
      "Iteration 194 \t| Loss = 0.4596839 \t| Accuracy = 0.8671875\n",
      "Iteration 195 \t| Loss = 0.33194968 \t| Accuracy = 0.90625\n",
      "Iteration 196 \t| Loss = 0.38369507 \t| Accuracy = 0.8828125\n",
      "Iteration 197 \t| Loss = 0.48056892 \t| Accuracy = 0.84375\n",
      "Iteration 198 \t| Loss = 0.41949317 \t| Accuracy = 0.8671875\n",
      "Iteration 199 \t| Loss = 0.5126633 \t| Accuracy = 0.8671875\n",
      "Iteration 200 \t| Loss = 0.5561305 \t| Accuracy = 0.828125\n",
      "Loss = 0.46829346 \t| Accuracy = 0.8515625\n",
      "Iteration 201 \t| Loss = 0.39927244 \t| Accuracy = 0.8984375\n",
      "Iteration 202 \t| Loss = 0.42187342 \t| Accuracy = 0.890625\n",
      "Iteration 203 \t| Loss = 0.4889235 \t| Accuracy = 0.828125\n",
      "Iteration 204 \t| Loss = 0.35441673 \t| Accuracy = 0.875\n",
      "Iteration 205 \t| Loss = 0.3535413 \t| Accuracy = 0.890625\n",
      "Iteration 206 \t| Loss = 0.38895637 \t| Accuracy = 0.8671875\n",
      "Iteration 207 \t| Loss = 0.4898801 \t| Accuracy = 0.8515625\n",
      "Iteration 208 \t| Loss = 0.29658008 \t| Accuracy = 0.9296875\n",
      "Iteration 209 \t| Loss = 0.37292027 \t| Accuracy = 0.875\n",
      "Iteration 210 \t| Loss = 0.44981715 \t| Accuracy = 0.8359375\n",
      "Iteration 211 \t| Loss = 0.2759341 \t| Accuracy = 0.90625\n",
      "Iteration 212 \t| Loss = 0.49861377 \t| Accuracy = 0.90625\n",
      "Iteration 213 \t| Loss = 0.41860372 \t| Accuracy = 0.8671875\n",
      "Iteration 214 \t| Loss = 0.36085692 \t| Accuracy = 0.8515625\n",
      "Iteration 215 \t| Loss = 0.34426075 \t| Accuracy = 0.90625\n",
      "Iteration 216 \t| Loss = 0.35866624 \t| Accuracy = 0.875\n",
      "Iteration 217 \t| Loss = 0.41704607 \t| Accuracy = 0.8828125\n",
      "Iteration 218 \t| Loss = 0.5168262 \t| Accuracy = 0.84375\n",
      "Iteration 219 \t| Loss = 0.4314373 \t| Accuracy = 0.8828125\n",
      "Iteration 220 \t| Loss = 0.3607752 \t| Accuracy = 0.8984375\n",
      "Iteration 221 \t| Loss = 0.34357405 \t| Accuracy = 0.875\n",
      "Iteration 222 \t| Loss = 0.5559915 \t| Accuracy = 0.8203125\n",
      "Iteration 223 \t| Loss = 0.37781447 \t| Accuracy = 0.8984375\n",
      "Iteration 224 \t| Loss = 0.3098689 \t| Accuracy = 0.9140625\n",
      "Iteration 225 \t| Loss = 0.37173957 \t| Accuracy = 0.8828125\n",
      "Iteration 226 \t| Loss = 0.49361205 \t| Accuracy = 0.8828125\n",
      "Iteration 227 \t| Loss = 0.26815864 \t| Accuracy = 0.9453125\n",
      "Iteration 228 \t| Loss = 0.3166362 \t| Accuracy = 0.921875\n",
      "Iteration 229 \t| Loss = 0.38003156 \t| Accuracy = 0.890625\n",
      "Iteration 230 \t| Loss = 0.46657953 \t| Accuracy = 0.8515625\n",
      "Iteration 231 \t| Loss = 0.25614578 \t| Accuracy = 0.90625\n",
      "Iteration 232 \t| Loss = 0.27341315 \t| Accuracy = 0.9296875\n",
      "Iteration 233 \t| Loss = 0.25753602 \t| Accuracy = 0.9140625\n",
      "Iteration 234 \t| Loss = 0.28806728 \t| Accuracy = 0.921875\n",
      "Iteration 235 \t| Loss = 0.29380962 \t| Accuracy = 0.90625\n",
      "Iteration 236 \t| Loss = 0.39966488 \t| Accuracy = 0.8671875\n",
      "Iteration 237 \t| Loss = 0.43371004 \t| Accuracy = 0.875\n",
      "Iteration 238 \t| Loss = 0.42411357 \t| Accuracy = 0.8671875\n",
      "Iteration 239 \t| Loss = 0.3448572 \t| Accuracy = 0.875\n",
      "Iteration 240 \t| Loss = 0.3329652 \t| Accuracy = 0.875\n",
      "Iteration 241 \t| Loss = 0.36463606 \t| Accuracy = 0.890625\n",
      "Iteration 242 \t| Loss = 0.5172572 \t| Accuracy = 0.8671875\n",
      "Iteration 243 \t| Loss = 0.38128924 \t| Accuracy = 0.8671875\n",
      "Iteration 244 \t| Loss = 0.27321917 \t| Accuracy = 0.9296875\n",
      "Iteration 245 \t| Loss = 0.43166724 \t| Accuracy = 0.8828125\n",
      "Iteration 246 \t| Loss = 0.4889676 \t| Accuracy = 0.859375\n",
      "Iteration 247 \t| Loss = 0.46156013 \t| Accuracy = 0.8046875\n",
      "Iteration 248 \t| Loss = 0.42717195 \t| Accuracy = 0.8671875\n",
      "Iteration 249 \t| Loss = 0.5187892 \t| Accuracy = 0.8359375\n",
      "Iteration 250 \t| Loss = 0.54154336 \t| Accuracy = 0.859375\n",
      "Iteration 251 \t| Loss = 0.5675008 \t| Accuracy = 0.8671875\n",
      "Iteration 252 \t| Loss = 0.62525177 \t| Accuracy = 0.84375\n",
      "Iteration 253 \t| Loss = 0.21849102 \t| Accuracy = 0.9609375\n",
      "Iteration 254 \t| Loss = 0.371167 \t| Accuracy = 0.8828125\n",
      "Iteration 255 \t| Loss = 0.38877285 \t| Accuracy = 0.8828125\n",
      "Iteration 256 \t| Loss = 0.32904992 \t| Accuracy = 0.9140625\n",
      "Iteration 257 \t| Loss = 0.31682944 \t| Accuracy = 0.8984375\n",
      "Iteration 258 \t| Loss = 0.40141433 \t| Accuracy = 0.8828125\n",
      "Iteration 259 \t| Loss = 0.36940187 \t| Accuracy = 0.90625\n",
      "Iteration 260 \t| Loss = 0.21740648 \t| Accuracy = 0.9453125\n",
      "Iteration 261 \t| Loss = 0.40166706 \t| Accuracy = 0.8984375\n",
      "Iteration 262 \t| Loss = 0.39811024 \t| Accuracy = 0.8984375\n",
      "Iteration 263 \t| Loss = 0.39249402 \t| Accuracy = 0.890625\n",
      "Iteration 264 \t| Loss = 0.39470953 \t| Accuracy = 0.8828125\n",
      "Iteration 265 \t| Loss = 0.3558908 \t| Accuracy = 0.890625\n",
      "Iteration 266 \t| Loss = 0.25939763 \t| Accuracy = 0.9375\n",
      "Iteration 267 \t| Loss = 0.2214704 \t| Accuracy = 0.9375\n",
      "Iteration 268 \t| Loss = 0.4184301 \t| Accuracy = 0.890625\n",
      "Iteration 269 \t| Loss = 0.38775218 \t| Accuracy = 0.8671875\n",
      "Iteration 270 \t| Loss = 0.38415235 \t| Accuracy = 0.90625\n",
      "Iteration 271 \t| Loss = 0.44750783 \t| Accuracy = 0.8828125\n",
      "Iteration 272 \t| Loss = 0.48855436 \t| Accuracy = 0.8671875\n",
      "Iteration 273 \t| Loss = 0.3362807 \t| Accuracy = 0.8984375\n",
      "Iteration 274 \t| Loss = 0.20270838 \t| Accuracy = 0.9453125\n",
      "Iteration 275 \t| Loss = 0.3588366 \t| Accuracy = 0.8984375\n",
      "Iteration 276 \t| Loss = 0.34214583 \t| Accuracy = 0.8828125\n",
      "Iteration 277 \t| Loss = 0.35597408 \t| Accuracy = 0.9140625\n",
      "Iteration 278 \t| Loss = 0.37912887 \t| Accuracy = 0.9140625\n",
      "Iteration 279 \t| Loss = 0.5955168 \t| Accuracy = 0.8359375\n",
      "Iteration 280 \t| Loss = 0.20454286 \t| Accuracy = 0.9375\n",
      "Iteration 281 \t| Loss = 0.5663512 \t| Accuracy = 0.8515625\n",
      "Iteration 282 \t| Loss = 0.36406326 \t| Accuracy = 0.8828125\n",
      "Iteration 283 \t| Loss = 0.40959197 \t| Accuracy = 0.8828125\n",
      "Iteration 284 \t| Loss = 0.30273837 \t| Accuracy = 0.890625\n",
      "Iteration 285 \t| Loss = 0.5402926 \t| Accuracy = 0.890625\n",
      "Iteration 286 \t| Loss = 0.42999774 \t| Accuracy = 0.890625\n",
      "Iteration 287 \t| Loss = 0.4399448 \t| Accuracy = 0.859375\n",
      "Iteration 288 \t| Loss = 0.2993734 \t| Accuracy = 0.921875\n",
      "Iteration 289 \t| Loss = 0.39110017 \t| Accuracy = 0.84375\n",
      "Iteration 290 \t| Loss = 0.16136669 \t| Accuracy = 0.9765625\n",
      "Iteration 291 \t| Loss = 0.282607 \t| Accuracy = 0.921875\n",
      "Iteration 292 \t| Loss = 0.40662974 \t| Accuracy = 0.8515625\n",
      "Iteration 293 \t| Loss = 0.33299062 \t| Accuracy = 0.9296875\n",
      "Iteration 294 \t| Loss = 0.33245552 \t| Accuracy = 0.875\n",
      "Iteration 295 \t| Loss = 0.3789379 \t| Accuracy = 0.890625\n",
      "Iteration 296 \t| Loss = 0.3798671 \t| Accuracy = 0.8984375\n",
      "Iteration 297 \t| Loss = 0.2852971 \t| Accuracy = 0.8828125\n",
      "Iteration 298 \t| Loss = 0.36874864 \t| Accuracy = 0.8828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 299 \t| Loss = 0.34446466 \t| Accuracy = 0.8984375\n",
      "Iteration 300 \t| Loss = 0.26440865 \t| Accuracy = 0.9140625\n",
      "Loss = 0.31822464 \t| Accuracy = 0.9375\n",
      "Iteration 301 \t| Loss = 0.28981674 \t| Accuracy = 0.9140625\n",
      "Iteration 302 \t| Loss = 0.28266978 \t| Accuracy = 0.8984375\n",
      "Iteration 303 \t| Loss = 0.34305224 \t| Accuracy = 0.875\n",
      "Iteration 304 \t| Loss = 0.3430683 \t| Accuracy = 0.890625\n",
      "Iteration 305 \t| Loss = 0.2992872 \t| Accuracy = 0.90625\n",
      "Iteration 306 \t| Loss = 0.58094627 \t| Accuracy = 0.8359375\n",
      "Iteration 307 \t| Loss = 0.31132275 \t| Accuracy = 0.9140625\n",
      "Iteration 308 \t| Loss = 0.2675435 \t| Accuracy = 0.9296875\n",
      "Iteration 309 \t| Loss = 0.37450805 \t| Accuracy = 0.8984375\n",
      "Iteration 310 \t| Loss = 0.4240428 \t| Accuracy = 0.84375\n",
      "Iteration 311 \t| Loss = 0.3157211 \t| Accuracy = 0.890625\n",
      "Iteration 312 \t| Loss = 0.4536786 \t| Accuracy = 0.8828125\n",
      "Iteration 313 \t| Loss = 0.42648387 \t| Accuracy = 0.890625\n",
      "Iteration 314 \t| Loss = 0.27679783 \t| Accuracy = 0.921875\n",
      "Iteration 315 \t| Loss = 0.33828503 \t| Accuracy = 0.9140625\n",
      "Iteration 316 \t| Loss = 0.44483727 \t| Accuracy = 0.8828125\n",
      "Iteration 317 \t| Loss = 0.5222912 \t| Accuracy = 0.859375\n",
      "Iteration 318 \t| Loss = 0.46878186 \t| Accuracy = 0.859375\n",
      "Iteration 319 \t| Loss = 0.31432807 \t| Accuracy = 0.875\n",
      "Iteration 320 \t| Loss = 0.3683179 \t| Accuracy = 0.8515625\n",
      "Iteration 321 \t| Loss = 0.399461 \t| Accuracy = 0.8671875\n",
      "Iteration 322 \t| Loss = 0.21825986 \t| Accuracy = 0.9609375\n",
      "Iteration 323 \t| Loss = 0.40633833 \t| Accuracy = 0.859375\n",
      "Iteration 324 \t| Loss = 0.23737556 \t| Accuracy = 0.921875\n",
      "Iteration 325 \t| Loss = 0.26537043 \t| Accuracy = 0.90625\n",
      "Iteration 326 \t| Loss = 0.3302375 \t| Accuracy = 0.875\n",
      "Iteration 327 \t| Loss = 0.27776068 \t| Accuracy = 0.9140625\n",
      "Iteration 328 \t| Loss = 0.3459584 \t| Accuracy = 0.890625\n",
      "Iteration 329 \t| Loss = 0.5742138 \t| Accuracy = 0.8359375\n",
      "Iteration 330 \t| Loss = 0.23670581 \t| Accuracy = 0.90625\n",
      "Iteration 331 \t| Loss = 0.46876082 \t| Accuracy = 0.8671875\n",
      "Iteration 332 \t| Loss = 0.3140962 \t| Accuracy = 0.90625\n",
      "Iteration 333 \t| Loss = 0.31543863 \t| Accuracy = 0.890625\n",
      "Iteration 334 \t| Loss = 0.296097 \t| Accuracy = 0.9296875\n",
      "Iteration 335 \t| Loss = 0.39699936 \t| Accuracy = 0.875\n",
      "Iteration 336 \t| Loss = 0.26626563 \t| Accuracy = 0.90625\n",
      "Iteration 337 \t| Loss = 0.25943986 \t| Accuracy = 0.90625\n",
      "Iteration 338 \t| Loss = 0.32564437 \t| Accuracy = 0.90625\n",
      "Iteration 339 \t| Loss = 0.3420893 \t| Accuracy = 0.890625\n",
      "Iteration 340 \t| Loss = 0.20925917 \t| Accuracy = 0.9453125\n",
      "Iteration 341 \t| Loss = 0.4011411 \t| Accuracy = 0.8828125\n",
      "Iteration 342 \t| Loss = 0.31262606 \t| Accuracy = 0.9140625\n",
      "Iteration 343 \t| Loss = 0.34506863 \t| Accuracy = 0.890625\n",
      "Iteration 344 \t| Loss = 0.280078 \t| Accuracy = 0.8984375\n",
      "Iteration 345 \t| Loss = 0.36195022 \t| Accuracy = 0.890625\n",
      "Iteration 346 \t| Loss = 0.31244332 \t| Accuracy = 0.921875\n",
      "Iteration 347 \t| Loss = 0.45658267 \t| Accuracy = 0.84375\n",
      "Iteration 348 \t| Loss = 0.30452567 \t| Accuracy = 0.921875\n",
      "Iteration 349 \t| Loss = 0.44233102 \t| Accuracy = 0.8515625\n",
      "Iteration 350 \t| Loss = 0.3147258 \t| Accuracy = 0.9453125\n",
      "Iteration 351 \t| Loss = 0.26219565 \t| Accuracy = 0.9609375\n",
      "Iteration 352 \t| Loss = 0.57717687 \t| Accuracy = 0.8359375\n",
      "Iteration 353 \t| Loss = 0.39110878 \t| Accuracy = 0.890625\n",
      "Iteration 354 \t| Loss = 0.38864934 \t| Accuracy = 0.90625\n",
      "Iteration 355 \t| Loss = 0.39830416 \t| Accuracy = 0.8828125\n",
      "Iteration 356 \t| Loss = 0.50270617 \t| Accuracy = 0.8828125\n",
      "Iteration 357 \t| Loss = 0.25810006 \t| Accuracy = 0.90625\n",
      "Iteration 358 \t| Loss = 0.39931405 \t| Accuracy = 0.84375\n",
      "Iteration 359 \t| Loss = 0.3093975 \t| Accuracy = 0.90625\n",
      "Iteration 360 \t| Loss = 0.29169166 \t| Accuracy = 0.90625\n",
      "Iteration 361 \t| Loss = 0.2572485 \t| Accuracy = 0.921875\n",
      "Iteration 362 \t| Loss = 0.28487816 \t| Accuracy = 0.9453125\n",
      "Iteration 363 \t| Loss = 0.19403017 \t| Accuracy = 0.9375\n",
      "Iteration 364 \t| Loss = 0.40872276 \t| Accuracy = 0.9140625\n",
      "Iteration 365 \t| Loss = 0.41240546 \t| Accuracy = 0.8984375\n",
      "Iteration 366 \t| Loss = 0.32239884 \t| Accuracy = 0.8984375\n",
      "Iteration 367 \t| Loss = 0.30661625 \t| Accuracy = 0.90625\n",
      "Iteration 368 \t| Loss = 0.29982907 \t| Accuracy = 0.8984375\n",
      "Iteration 369 \t| Loss = 0.24066775 \t| Accuracy = 0.9296875\n",
      "Iteration 370 \t| Loss = 0.41699204 \t| Accuracy = 0.90625\n",
      "Iteration 371 \t| Loss = 0.33980182 \t| Accuracy = 0.9140625\n",
      "Iteration 372 \t| Loss = 0.42847258 \t| Accuracy = 0.890625\n",
      "Iteration 373 \t| Loss = 0.3978564 \t| Accuracy = 0.875\n",
      "Iteration 374 \t| Loss = 0.28700927 \t| Accuracy = 0.9140625\n",
      "Iteration 375 \t| Loss = 0.29070103 \t| Accuracy = 0.9375\n",
      "Iteration 376 \t| Loss = 0.28135085 \t| Accuracy = 0.921875\n",
      "Iteration 377 \t| Loss = 0.4315402 \t| Accuracy = 0.8671875\n",
      "Iteration 378 \t| Loss = 0.33790353 \t| Accuracy = 0.9140625\n",
      "Iteration 379 \t| Loss = 0.34558615 \t| Accuracy = 0.9140625\n",
      "Iteration 380 \t| Loss = 0.37213078 \t| Accuracy = 0.875\n",
      "Iteration 381 \t| Loss = 0.39003548 \t| Accuracy = 0.8984375\n",
      "Iteration 382 \t| Loss = 0.26675135 \t| Accuracy = 0.9140625\n",
      "Iteration 383 \t| Loss = 0.38904953 \t| Accuracy = 0.875\n",
      "Iteration 384 \t| Loss = 0.26887763 \t| Accuracy = 0.890625\n",
      "Iteration 385 \t| Loss = 0.27731344 \t| Accuracy = 0.8984375\n",
      "Iteration 386 \t| Loss = 0.42739046 \t| Accuracy = 0.8984375\n",
      "Iteration 387 \t| Loss = 0.34404975 \t| Accuracy = 0.875\n",
      "Iteration 388 \t| Loss = 0.33764756 \t| Accuracy = 0.875\n",
      "Iteration 389 \t| Loss = 0.37852603 \t| Accuracy = 0.8984375\n",
      "Iteration 390 \t| Loss = 0.36700165 \t| Accuracy = 0.90625\n",
      "Iteration 391 \t| Loss = 0.40942183 \t| Accuracy = 0.90625\n",
      "Iteration 392 \t| Loss = 0.2473836 \t| Accuracy = 0.9453125\n",
      "Iteration 393 \t| Loss = 0.4576254 \t| Accuracy = 0.8828125\n",
      "Iteration 394 \t| Loss = 0.42388734 \t| Accuracy = 0.8828125\n",
      "Iteration 395 \t| Loss = 0.18967117 \t| Accuracy = 0.9375\n",
      "Iteration 396 \t| Loss = 0.24107681 \t| Accuracy = 0.921875\n",
      "Iteration 397 \t| Loss = 0.27531588 \t| Accuracy = 0.9375\n",
      "Iteration 398 \t| Loss = 0.36532563 \t| Accuracy = 0.8984375\n",
      "Iteration 399 \t| Loss = 0.25417018 \t| Accuracy = 0.9375\n",
      "Iteration 400 \t| Loss = 0.41219407 \t| Accuracy = 0.9140625\n",
      "Loss = 0.31284425 \t| Accuracy = 0.8828125\n",
      "Iteration 401 \t| Loss = 0.5183471 \t| Accuracy = 0.828125\n",
      "Iteration 402 \t| Loss = 0.28489125 \t| Accuracy = 0.90625\n",
      "Iteration 403 \t| Loss = 0.2964896 \t| Accuracy = 0.8984375\n",
      "Iteration 404 \t| Loss = 0.5077117 \t| Accuracy = 0.8671875\n",
      "Iteration 405 \t| Loss = 0.48929548 \t| Accuracy = 0.8515625\n",
      "Iteration 406 \t| Loss = 0.32456157 \t| Accuracy = 0.8828125\n",
      "Iteration 407 \t| Loss = 0.34588894 \t| Accuracy = 0.921875\n",
      "Iteration 408 \t| Loss = 0.35188916 \t| Accuracy = 0.9296875\n",
      "Iteration 409 \t| Loss = 0.21445478 \t| Accuracy = 0.9140625\n",
      "Iteration 410 \t| Loss = 0.2644662 \t| Accuracy = 0.9453125\n",
      "Iteration 411 \t| Loss = 0.42470926 \t| Accuracy = 0.8671875\n",
      "Iteration 412 \t| Loss = 0.4432729 \t| Accuracy = 0.84375\n",
      "Iteration 413 \t| Loss = 0.3412462 \t| Accuracy = 0.921875\n",
      "Iteration 414 \t| Loss = 0.3125776 \t| Accuracy = 0.8828125\n",
      "Iteration 415 \t| Loss = 0.27505073 \t| Accuracy = 0.9296875\n",
      "Iteration 416 \t| Loss = 0.2628055 \t| Accuracy = 0.9375\n",
      "Iteration 417 \t| Loss = 0.41998738 \t| Accuracy = 0.9140625\n",
      "Iteration 418 \t| Loss = 0.33225223 \t| Accuracy = 0.921875\n",
      "Iteration 419 \t| Loss = 0.3074426 \t| Accuracy = 0.921875\n",
      "Iteration 420 \t| Loss = 0.24234071 \t| Accuracy = 0.921875\n",
      "Iteration 421 \t| Loss = 0.3763848 \t| Accuracy = 0.8828125\n",
      "Iteration 422 \t| Loss = 0.275572 \t| Accuracy = 0.9375\n",
      "Iteration 423 \t| Loss = 0.35838982 \t| Accuracy = 0.9296875\n",
      "Iteration 424 \t| Loss = 0.39224887 \t| Accuracy = 0.90625\n",
      "Iteration 425 \t| Loss = 0.2977467 \t| Accuracy = 0.921875\n",
      "Iteration 426 \t| Loss = 0.28531766 \t| Accuracy = 0.921875\n",
      "Iteration 427 \t| Loss = 0.43653536 \t| Accuracy = 0.890625\n",
      "Iteration 428 \t| Loss = 0.31664148 \t| Accuracy = 0.9375\n",
      "Iteration 429 \t| Loss = 0.29572266 \t| Accuracy = 0.890625\n",
      "Iteration 430 \t| Loss = 0.3917367 \t| Accuracy = 0.890625\n",
      "Iteration 431 \t| Loss = 0.28815877 \t| Accuracy = 0.9140625\n",
      "Iteration 432 \t| Loss = 0.25528997 \t| Accuracy = 0.921875\n",
      "Iteration 433 \t| Loss = 0.42231938 \t| Accuracy = 0.8984375\n",
      "Iteration 434 \t| Loss = 0.31510395 \t| Accuracy = 0.9140625\n",
      "Iteration 435 \t| Loss = 0.39185676 \t| Accuracy = 0.8828125\n",
      "Iteration 436 \t| Loss = 0.30142093 \t| Accuracy = 0.921875\n",
      "Iteration 437 \t| Loss = 0.4055742 \t| Accuracy = 0.890625\n",
      "Iteration 438 \t| Loss = 0.3364411 \t| Accuracy = 0.90625\n",
      "Iteration 439 \t| Loss = 0.3803056 \t| Accuracy = 0.8984375\n",
      "Iteration 440 \t| Loss = 0.27456737 \t| Accuracy = 0.9296875\n",
      "Iteration 441 \t| Loss = 0.49773037 \t| Accuracy = 0.8828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 442 \t| Loss = 0.3489517 \t| Accuracy = 0.8984375\n",
      "Iteration 443 \t| Loss = 0.3342321 \t| Accuracy = 0.890625\n",
      "Iteration 444 \t| Loss = 0.34225488 \t| Accuracy = 0.8984375\n",
      "Iteration 445 \t| Loss = 0.3380007 \t| Accuracy = 0.8984375\n",
      "Iteration 446 \t| Loss = 0.26514667 \t| Accuracy = 0.9375\n",
      "Iteration 447 \t| Loss = 0.3453601 \t| Accuracy = 0.8828125\n",
      "Iteration 448 \t| Loss = 0.30789655 \t| Accuracy = 0.8984375\n",
      "Iteration 449 \t| Loss = 0.31518486 \t| Accuracy = 0.921875\n",
      "Iteration 450 \t| Loss = 0.3001306 \t| Accuracy = 0.921875\n",
      "Iteration 451 \t| Loss = 0.21328637 \t| Accuracy = 0.9453125\n",
      "Iteration 452 \t| Loss = 0.1473242 \t| Accuracy = 0.96875\n",
      "Iteration 453 \t| Loss = 0.36725456 \t| Accuracy = 0.8984375\n",
      "Iteration 454 \t| Loss = 0.21360028 \t| Accuracy = 0.9453125\n",
      "Iteration 455 \t| Loss = 0.30170822 \t| Accuracy = 0.9375\n",
      "Iteration 456 \t| Loss = 0.46194023 \t| Accuracy = 0.8359375\n",
      "Iteration 457 \t| Loss = 0.35217702 \t| Accuracy = 0.8984375\n",
      "Iteration 458 \t| Loss = 0.21862645 \t| Accuracy = 0.953125\n",
      "Iteration 459 \t| Loss = 0.44988042 \t| Accuracy = 0.875\n",
      "Iteration 460 \t| Loss = 0.40615547 \t| Accuracy = 0.859375\n",
      "Iteration 461 \t| Loss = 0.33385015 \t| Accuracy = 0.9296875\n",
      "Iteration 462 \t| Loss = 0.3266955 \t| Accuracy = 0.90625\n",
      "Iteration 463 \t| Loss = 0.2737715 \t| Accuracy = 0.9296875\n",
      "Iteration 464 \t| Loss = 0.30238152 \t| Accuracy = 0.9296875\n",
      "Iteration 465 \t| Loss = 0.31751797 \t| Accuracy = 0.8828125\n",
      "Iteration 466 \t| Loss = 0.373004 \t| Accuracy = 0.890625\n",
      "Iteration 467 \t| Loss = 0.38692725 \t| Accuracy = 0.9375\n",
      "Iteration 468 \t| Loss = 0.4414057 \t| Accuracy = 0.875\n",
      "Iteration 469 \t| Loss = 0.26774663 \t| Accuracy = 0.8984375\n",
      "Iteration 470 \t| Loss = 0.36779726 \t| Accuracy = 0.9140625\n",
      "Iteration 471 \t| Loss = 0.3413754 \t| Accuracy = 0.9140625\n",
      "Iteration 472 \t| Loss = 0.1929527 \t| Accuracy = 0.9140625\n",
      "Iteration 473 \t| Loss = 0.3230547 \t| Accuracy = 0.9453125\n",
      "Iteration 474 \t| Loss = 0.2811827 \t| Accuracy = 0.90625\n",
      "Iteration 475 \t| Loss = 0.45325857 \t| Accuracy = 0.859375\n",
      "Iteration 476 \t| Loss = 0.38851047 \t| Accuracy = 0.8671875\n",
      "Iteration 477 \t| Loss = 0.26714802 \t| Accuracy = 0.9140625\n",
      "Iteration 478 \t| Loss = 0.18971771 \t| Accuracy = 0.9296875\n",
      "Iteration 479 \t| Loss = 0.22045231 \t| Accuracy = 0.9453125\n",
      "Iteration 480 \t| Loss = 0.2774954 \t| Accuracy = 0.921875\n",
      "Iteration 481 \t| Loss = 0.29602867 \t| Accuracy = 0.921875\n",
      "Iteration 482 \t| Loss = 0.40464574 \t| Accuracy = 0.9296875\n",
      "Iteration 483 \t| Loss = 0.426059 \t| Accuracy = 0.9296875\n",
      "Iteration 484 \t| Loss = 0.36224788 \t| Accuracy = 0.8828125\n",
      "Iteration 485 \t| Loss = 0.419858 \t| Accuracy = 0.859375\n",
      "Iteration 486 \t| Loss = 0.28291693 \t| Accuracy = 0.8984375\n",
      "Iteration 487 \t| Loss = 0.3713699 \t| Accuracy = 0.8828125\n",
      "Iteration 488 \t| Loss = 0.33538696 \t| Accuracy = 0.9140625\n",
      "Iteration 489 \t| Loss = 0.2771907 \t| Accuracy = 0.8984375\n",
      "Iteration 490 \t| Loss = 0.19558181 \t| Accuracy = 0.9453125\n",
      "Iteration 491 \t| Loss = 0.36186707 \t| Accuracy = 0.921875\n",
      "Iteration 492 \t| Loss = 0.25695658 \t| Accuracy = 0.921875\n",
      "Iteration 493 \t| Loss = 0.21345006 \t| Accuracy = 0.921875\n",
      "Iteration 494 \t| Loss = 0.33750397 \t| Accuracy = 0.8984375\n",
      "Iteration 495 \t| Loss = 0.29484075 \t| Accuracy = 0.9140625\n",
      "Iteration 496 \t| Loss = 0.28234887 \t| Accuracy = 0.9140625\n",
      "Iteration 497 \t| Loss = 0.3819474 \t| Accuracy = 0.8984375\n",
      "Iteration 498 \t| Loss = 0.3465457 \t| Accuracy = 0.875\n",
      "Iteration 499 \t| Loss = 0.329611 \t| Accuracy = 0.90625\n",
      "Iteration 500 \t| Loss = 0.40078005 \t| Accuracy = 0.890625\n",
      "Loss = 0.43499744 \t| Accuracy = 0.8515625\n",
      "Iteration 501 \t| Loss = 0.42534888 \t| Accuracy = 0.8828125\n",
      "Iteration 502 \t| Loss = 0.30880165 \t| Accuracy = 0.8984375\n",
      "Iteration 503 \t| Loss = 0.29243928 \t| Accuracy = 0.9140625\n",
      "Iteration 504 \t| Loss = 0.23581724 \t| Accuracy = 0.9296875\n",
      "Iteration 505 \t| Loss = 0.32800674 \t| Accuracy = 0.890625\n",
      "Iteration 506 \t| Loss = 0.23687252 \t| Accuracy = 0.9453125\n",
      "Iteration 507 \t| Loss = 0.20991088 \t| Accuracy = 0.9296875\n",
      "Iteration 508 \t| Loss = 0.36824018 \t| Accuracy = 0.890625\n",
      "Iteration 509 \t| Loss = 0.25125334 \t| Accuracy = 0.953125\n",
      "Iteration 510 \t| Loss = 0.4081275 \t| Accuracy = 0.8984375\n",
      "Iteration 511 \t| Loss = 0.44201252 \t| Accuracy = 0.859375\n",
      "Iteration 512 \t| Loss = 0.43052924 \t| Accuracy = 0.8828125\n",
      "Iteration 513 \t| Loss = 0.4121639 \t| Accuracy = 0.875\n",
      "Iteration 514 \t| Loss = 0.3009204 \t| Accuracy = 0.9453125\n",
      "Iteration 515 \t| Loss = 0.3955439 \t| Accuracy = 0.921875\n",
      "Iteration 516 \t| Loss = 0.34206256 \t| Accuracy = 0.8828125\n",
      "Iteration 517 \t| Loss = 0.36593738 \t| Accuracy = 0.8671875\n",
      "Iteration 518 \t| Loss = 0.21345438 \t| Accuracy = 0.9296875\n",
      "Iteration 519 \t| Loss = 0.22910145 \t| Accuracy = 0.921875\n",
      "Iteration 520 \t| Loss = 0.42040935 \t| Accuracy = 0.8984375\n",
      "Iteration 521 \t| Loss = 0.34978577 \t| Accuracy = 0.9140625\n",
      "Iteration 522 \t| Loss = 0.19842255 \t| Accuracy = 0.9375\n",
      "Iteration 523 \t| Loss = 0.26568323 \t| Accuracy = 0.90625\n",
      "Iteration 524 \t| Loss = 0.21936141 \t| Accuracy = 0.9375\n",
      "Iteration 525 \t| Loss = 0.26961267 \t| Accuracy = 0.921875\n",
      "Iteration 526 \t| Loss = 0.22334832 \t| Accuracy = 0.9296875\n",
      "Iteration 527 \t| Loss = 0.32806757 \t| Accuracy = 0.921875\n",
      "Iteration 528 \t| Loss = 0.22888277 \t| Accuracy = 0.9453125\n",
      "Iteration 529 \t| Loss = 0.22179936 \t| Accuracy = 0.9296875\n",
      "Iteration 530 \t| Loss = 0.2666884 \t| Accuracy = 0.90625\n",
      "Iteration 531 \t| Loss = 0.33968437 \t| Accuracy = 0.859375\n",
      "Iteration 532 \t| Loss = 0.3614648 \t| Accuracy = 0.8984375\n",
      "Iteration 533 \t| Loss = 0.3061216 \t| Accuracy = 0.9296875\n",
      "Iteration 534 \t| Loss = 0.2570759 \t| Accuracy = 0.9375\n",
      "Iteration 535 \t| Loss = 0.20845702 \t| Accuracy = 0.9375\n",
      "Iteration 536 \t| Loss = 0.3695243 \t| Accuracy = 0.90625\n",
      "Iteration 537 \t| Loss = 0.34893638 \t| Accuracy = 0.8984375\n",
      "Iteration 538 \t| Loss = 0.4184248 \t| Accuracy = 0.90625\n",
      "Iteration 539 \t| Loss = 0.26686785 \t| Accuracy = 0.921875\n",
      "Iteration 540 \t| Loss = 0.29191926 \t| Accuracy = 0.9140625\n",
      "Iteration 541 \t| Loss = 0.38313445 \t| Accuracy = 0.890625\n",
      "Iteration 542 \t| Loss = 0.275971 \t| Accuracy = 0.9453125\n",
      "Iteration 543 \t| Loss = 0.40799782 \t| Accuracy = 0.8984375\n",
      "Iteration 544 \t| Loss = 0.29824835 \t| Accuracy = 0.8984375\n",
      "Iteration 545 \t| Loss = 0.39009854 \t| Accuracy = 0.875\n",
      "Iteration 546 \t| Loss = 0.26154822 \t| Accuracy = 0.953125\n",
      "Iteration 547 \t| Loss = 0.23994195 \t| Accuracy = 0.921875\n",
      "Iteration 548 \t| Loss = 0.27546588 \t| Accuracy = 0.90625\n",
      "Iteration 549 \t| Loss = 0.30846697 \t| Accuracy = 0.921875\n",
      "Iteration 550 \t| Loss = 0.3622498 \t| Accuracy = 0.8984375\n",
      "Iteration 551 \t| Loss = 0.39759988 \t| Accuracy = 0.90625\n",
      "Iteration 552 \t| Loss = 0.33620095 \t| Accuracy = 0.90625\n",
      "Iteration 553 \t| Loss = 0.40971982 \t| Accuracy = 0.890625\n",
      "Iteration 554 \t| Loss = 0.36574882 \t| Accuracy = 0.890625\n",
      "Iteration 555 \t| Loss = 0.38507318 \t| Accuracy = 0.859375\n",
      "Iteration 556 \t| Loss = 0.49266 \t| Accuracy = 0.8671875\n",
      "Iteration 557 \t| Loss = 0.48927248 \t| Accuracy = 0.8984375\n",
      "Iteration 558 \t| Loss = 0.3331191 \t| Accuracy = 0.875\n",
      "Iteration 559 \t| Loss = 0.4477285 \t| Accuracy = 0.890625\n",
      "Iteration 560 \t| Loss = 0.39694583 \t| Accuracy = 0.875\n",
      "Iteration 561 \t| Loss = 0.21816611 \t| Accuracy = 0.9140625\n",
      "Iteration 562 \t| Loss = 0.40334526 \t| Accuracy = 0.890625\n",
      "Iteration 563 \t| Loss = 0.43904954 \t| Accuracy = 0.8828125\n",
      "Iteration 564 \t| Loss = 0.29939076 \t| Accuracy = 0.90625\n",
      "Iteration 565 \t| Loss = 0.45327383 \t| Accuracy = 0.8515625\n",
      "Iteration 566 \t| Loss = 0.34019777 \t| Accuracy = 0.890625\n",
      "Iteration 567 \t| Loss = 0.4400795 \t| Accuracy = 0.8984375\n",
      "Iteration 568 \t| Loss = 0.46257344 \t| Accuracy = 0.8515625\n",
      "Iteration 569 \t| Loss = 0.39304006 \t| Accuracy = 0.90625\n",
      "Iteration 570 \t| Loss = 0.39195588 \t| Accuracy = 0.8671875\n",
      "Iteration 571 \t| Loss = 0.32479066 \t| Accuracy = 0.875\n",
      "Iteration 572 \t| Loss = 0.34108454 \t| Accuracy = 0.8984375\n",
      "Iteration 573 \t| Loss = 0.2608902 \t| Accuracy = 0.9296875\n",
      "Iteration 574 \t| Loss = 0.25975192 \t| Accuracy = 0.9140625\n",
      "Iteration 575 \t| Loss = 0.37860885 \t| Accuracy = 0.8984375\n",
      "Iteration 576 \t| Loss = 0.35716078 \t| Accuracy = 0.8984375\n",
      "Iteration 577 \t| Loss = 0.28288466 \t| Accuracy = 0.9140625\n",
      "Iteration 578 \t| Loss = 0.24508837 \t| Accuracy = 0.9453125\n",
      "Iteration 579 \t| Loss = 0.21900448 \t| Accuracy = 0.9296875\n",
      "Iteration 580 \t| Loss = 0.36082476 \t| Accuracy = 0.8828125\n",
      "Iteration 581 \t| Loss = 0.27418312 \t| Accuracy = 0.890625\n",
      "Iteration 582 \t| Loss = 0.37212366 \t| Accuracy = 0.890625\n",
      "Iteration 583 \t| Loss = 0.2709934 \t| Accuracy = 0.9140625\n",
      "Iteration 584 \t| Loss = 0.24926358 \t| Accuracy = 0.9296875\n",
      "Iteration 585 \t| Loss = 0.22856785 \t| Accuracy = 0.9375\n",
      "Iteration 586 \t| Loss = 0.23497945 \t| Accuracy = 0.9453125\n",
      "Iteration 587 \t| Loss = 0.37090144 \t| Accuracy = 0.9140625\n",
      "Iteration 588 \t| Loss = 0.21224862 \t| Accuracy = 0.9296875\n",
      "Iteration 589 \t| Loss = 0.23347804 \t| Accuracy = 0.921875\n",
      "Iteration 590 \t| Loss = 0.40367502 \t| Accuracy = 0.9375\n",
      "Iteration 591 \t| Loss = 0.43864337 \t| Accuracy = 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 592 \t| Loss = 0.3756986 \t| Accuracy = 0.8359375\n",
      "Iteration 593 \t| Loss = 0.389326 \t| Accuracy = 0.90625\n",
      "Iteration 594 \t| Loss = 0.34170216 \t| Accuracy = 0.8984375\n",
      "Iteration 595 \t| Loss = 0.3263036 \t| Accuracy = 0.921875\n",
      "Iteration 596 \t| Loss = 0.2921024 \t| Accuracy = 0.9453125\n",
      "Iteration 597 \t| Loss = 0.41160363 \t| Accuracy = 0.8984375\n",
      "Iteration 598 \t| Loss = 0.38853243 \t| Accuracy = 0.90625\n",
      "Iteration 599 \t| Loss = 0.2449511 \t| Accuracy = 0.921875\n",
      "Iteration 600 \t| Loss = 0.29881898 \t| Accuracy = 0.921875\n",
      "Loss = 0.3279192 \t| Accuracy = 0.890625\n",
      "Iteration 601 \t| Loss = 0.43256712 \t| Accuracy = 0.8828125\n",
      "Iteration 602 \t| Loss = 0.22276533 \t| Accuracy = 0.9453125\n",
      "Iteration 603 \t| Loss = 0.31303397 \t| Accuracy = 0.890625\n",
      "Iteration 604 \t| Loss = 0.34874338 \t| Accuracy = 0.8828125\n",
      "Iteration 605 \t| Loss = 0.28698885 \t| Accuracy = 0.90625\n",
      "Iteration 606 \t| Loss = 0.3074788 \t| Accuracy = 0.890625\n",
      "Iteration 607 \t| Loss = 0.37961742 \t| Accuracy = 0.90625\n",
      "Iteration 608 \t| Loss = 0.27581638 \t| Accuracy = 0.90625\n",
      "Iteration 609 \t| Loss = 0.4073276 \t| Accuracy = 0.890625\n",
      "Iteration 610 \t| Loss = 0.33201024 \t| Accuracy = 0.921875\n",
      "Iteration 611 \t| Loss = 0.34007198 \t| Accuracy = 0.890625\n",
      "Iteration 612 \t| Loss = 0.29075158 \t| Accuracy = 0.9140625\n",
      "Iteration 613 \t| Loss = 0.20573011 \t| Accuracy = 0.9375\n",
      "Iteration 614 \t| Loss = 0.41814977 \t| Accuracy = 0.859375\n",
      "Iteration 615 \t| Loss = 0.22966228 \t| Accuracy = 0.9375\n",
      "Iteration 616 \t| Loss = 0.31607282 \t| Accuracy = 0.9296875\n",
      "Iteration 617 \t| Loss = 0.292198 \t| Accuracy = 0.90625\n",
      "Iteration 618 \t| Loss = 0.2182651 \t| Accuracy = 0.9296875\n",
      "Iteration 619 \t| Loss = 0.41946927 \t| Accuracy = 0.859375\n",
      "Iteration 620 \t| Loss = 0.39657152 \t| Accuracy = 0.8828125\n",
      "Iteration 621 \t| Loss = 0.25080058 \t| Accuracy = 0.921875\n",
      "Iteration 622 \t| Loss = 0.275122 \t| Accuracy = 0.9140625\n",
      "Iteration 623 \t| Loss = 0.39332283 \t| Accuracy = 0.90625\n",
      "Iteration 624 \t| Loss = 0.33568835 \t| Accuracy = 0.8984375\n",
      "Iteration 625 \t| Loss = 0.15465128 \t| Accuracy = 0.96875\n",
      "Iteration 626 \t| Loss = 0.36907136 \t| Accuracy = 0.9375\n",
      "Iteration 627 \t| Loss = 0.27998614 \t| Accuracy = 0.9140625\n",
      "Iteration 628 \t| Loss = 0.23533873 \t| Accuracy = 0.9375\n",
      "Iteration 629 \t| Loss = 0.33075348 \t| Accuracy = 0.890625\n",
      "Iteration 630 \t| Loss = 0.31461746 \t| Accuracy = 0.8984375\n",
      "Iteration 631 \t| Loss = 0.21217483 \t| Accuracy = 0.953125\n",
      "Iteration 632 \t| Loss = 0.30392265 \t| Accuracy = 0.9140625\n",
      "Iteration 633 \t| Loss = 0.3628636 \t| Accuracy = 0.90625\n",
      "Iteration 634 \t| Loss = 0.26692715 \t| Accuracy = 0.921875\n",
      "Iteration 635 \t| Loss = 0.2177676 \t| Accuracy = 0.9375\n",
      "Iteration 636 \t| Loss = 0.4588085 \t| Accuracy = 0.90625\n",
      "Iteration 637 \t| Loss = 0.47971827 \t| Accuracy = 0.8671875\n",
      "Iteration 638 \t| Loss = 0.3340093 \t| Accuracy = 0.921875\n",
      "Iteration 639 \t| Loss = 0.2530409 \t| Accuracy = 0.9140625\n",
      "Iteration 640 \t| Loss = 0.22644387 \t| Accuracy = 0.953125\n",
      "Iteration 641 \t| Loss = 0.26737463 \t| Accuracy = 0.9296875\n",
      "Iteration 642 \t| Loss = 0.37355694 \t| Accuracy = 0.8984375\n",
      "Iteration 643 \t| Loss = 0.28921822 \t| Accuracy = 0.8984375\n",
      "Iteration 644 \t| Loss = 0.3080998 \t| Accuracy = 0.9140625\n",
      "Iteration 645 \t| Loss = 0.24343854 \t| Accuracy = 0.9375\n",
      "Iteration 646 \t| Loss = 0.34052634 \t| Accuracy = 0.9140625\n",
      "Iteration 647 \t| Loss = 0.3000976 \t| Accuracy = 0.890625\n",
      "Iteration 648 \t| Loss = 0.30384356 \t| Accuracy = 0.9140625\n",
      "Iteration 649 \t| Loss = 0.28173268 \t| Accuracy = 0.90625\n",
      "Iteration 650 \t| Loss = 0.27013424 \t| Accuracy = 0.90625\n",
      "Iteration 651 \t| Loss = 0.26574743 \t| Accuracy = 0.8984375\n",
      "Iteration 652 \t| Loss = 0.19985692 \t| Accuracy = 0.9453125\n",
      "Iteration 653 \t| Loss = 0.40724146 \t| Accuracy = 0.8828125\n",
      "Iteration 654 \t| Loss = 0.23873386 \t| Accuracy = 0.90625\n",
      "Iteration 655 \t| Loss = 0.2921213 \t| Accuracy = 0.90625\n",
      "Iteration 656 \t| Loss = 0.2797634 \t| Accuracy = 0.9140625\n",
      "Iteration 657 \t| Loss = 0.42182845 \t| Accuracy = 0.890625\n",
      "Iteration 658 \t| Loss = 0.32944322 \t| Accuracy = 0.921875\n",
      "Iteration 659 \t| Loss = 0.17902826 \t| Accuracy = 0.9609375\n",
      "Iteration 660 \t| Loss = 0.31215394 \t| Accuracy = 0.8828125\n",
      "Iteration 661 \t| Loss = 0.25624314 \t| Accuracy = 0.9375\n",
      "Iteration 662 \t| Loss = 0.31394988 \t| Accuracy = 0.90625\n",
      "Iteration 663 \t| Loss = 0.3268227 \t| Accuracy = 0.890625\n",
      "Iteration 664 \t| Loss = 0.28508124 \t| Accuracy = 0.9375\n",
      "Iteration 665 \t| Loss = 0.3319985 \t| Accuracy = 0.9140625\n",
      "Iteration 666 \t| Loss = 0.27322218 \t| Accuracy = 0.921875\n",
      "Iteration 667 \t| Loss = 0.33610502 \t| Accuracy = 0.921875\n",
      "Iteration 668 \t| Loss = 0.24671979 \t| Accuracy = 0.90625\n",
      "Iteration 669 \t| Loss = 0.2401225 \t| Accuracy = 0.921875\n",
      "Iteration 670 \t| Loss = 0.37218726 \t| Accuracy = 0.8828125\n",
      "Iteration 671 \t| Loss = 0.25365722 \t| Accuracy = 0.9296875\n",
      "Iteration 672 \t| Loss = 0.43980357 \t| Accuracy = 0.859375\n",
      "Iteration 673 \t| Loss = 0.2704904 \t| Accuracy = 0.921875\n",
      "Iteration 674 \t| Loss = 0.3690145 \t| Accuracy = 0.9140625\n",
      "Iteration 675 \t| Loss = 0.23815176 \t| Accuracy = 0.9375\n",
      "Iteration 676 \t| Loss = 0.36502266 \t| Accuracy = 0.8671875\n",
      "Iteration 677 \t| Loss = 0.46536583 \t| Accuracy = 0.84375\n",
      "Iteration 678 \t| Loss = 0.437308 \t| Accuracy = 0.875\n",
      "Iteration 679 \t| Loss = 0.3782484 \t| Accuracy = 0.9140625\n",
      "Iteration 680 \t| Loss = 0.22771785 \t| Accuracy = 0.9140625\n",
      "Iteration 681 \t| Loss = 0.4123181 \t| Accuracy = 0.9140625\n",
      "Iteration 682 \t| Loss = 0.33696067 \t| Accuracy = 0.890625\n",
      "Iteration 683 \t| Loss = 0.22660142 \t| Accuracy = 0.9140625\n",
      "Iteration 684 \t| Loss = 0.45526263 \t| Accuracy = 0.8984375\n",
      "Iteration 685 \t| Loss = 0.33399695 \t| Accuracy = 0.890625\n",
      "Iteration 686 \t| Loss = 0.28025085 \t| Accuracy = 0.9296875\n",
      "Iteration 687 \t| Loss = 0.352997 \t| Accuracy = 0.8984375\n",
      "Iteration 688 \t| Loss = 0.34952724 \t| Accuracy = 0.8828125\n",
      "Iteration 689 \t| Loss = 0.35266554 \t| Accuracy = 0.90625\n",
      "Iteration 690 \t| Loss = 0.4527823 \t| Accuracy = 0.8359375\n",
      "Iteration 691 \t| Loss = 0.29231828 \t| Accuracy = 0.90625\n",
      "Iteration 692 \t| Loss = 0.20812777 \t| Accuracy = 0.9453125\n",
      "Iteration 693 \t| Loss = 0.2707728 \t| Accuracy = 0.8984375\n",
      "Iteration 694 \t| Loss = 0.36841705 \t| Accuracy = 0.8671875\n",
      "Iteration 695 \t| Loss = 0.18751408 \t| Accuracy = 0.953125\n",
      "Iteration 696 \t| Loss = 0.2726019 \t| Accuracy = 0.9140625\n",
      "Iteration 697 \t| Loss = 0.40937635 \t| Accuracy = 0.8984375\n",
      "Iteration 698 \t| Loss = 0.28546718 \t| Accuracy = 0.9140625\n",
      "Iteration 699 \t| Loss = 0.3258815 \t| Accuracy = 0.90625\n",
      "Iteration 700 \t| Loss = 0.31422508 \t| Accuracy = 0.8984375\n",
      "Loss = 0.3168148 \t| Accuracy = 0.875\n",
      "Iteration 701 \t| Loss = 0.20815335 \t| Accuracy = 0.9296875\n",
      "Iteration 702 \t| Loss = 0.3176677 \t| Accuracy = 0.8828125\n",
      "Iteration 703 \t| Loss = 0.3556249 \t| Accuracy = 0.875\n",
      "Iteration 704 \t| Loss = 0.25599402 \t| Accuracy = 0.9375\n",
      "Iteration 705 \t| Loss = 0.3306557 \t| Accuracy = 0.9140625\n",
      "Iteration 706 \t| Loss = 0.40597314 \t| Accuracy = 0.890625\n",
      "Iteration 707 \t| Loss = 0.32129616 \t| Accuracy = 0.921875\n",
      "Iteration 708 \t| Loss = 0.29121828 \t| Accuracy = 0.90625\n",
      "Iteration 709 \t| Loss = 0.56290734 \t| Accuracy = 0.875\n",
      "Iteration 710 \t| Loss = 0.32448792 \t| Accuracy = 0.890625\n",
      "Iteration 711 \t| Loss = 0.23266211 \t| Accuracy = 0.9453125\n",
      "Iteration 712 \t| Loss = 0.2273953 \t| Accuracy = 0.9375\n",
      "Iteration 713 \t| Loss = 0.22294705 \t| Accuracy = 0.9453125\n",
      "Iteration 714 \t| Loss = 0.36822748 \t| Accuracy = 0.9140625\n",
      "Iteration 715 \t| Loss = 0.3010824 \t| Accuracy = 0.90625\n",
      "Iteration 716 \t| Loss = 0.33536348 \t| Accuracy = 0.9296875\n",
      "Iteration 717 \t| Loss = 0.35550267 \t| Accuracy = 0.8828125\n",
      "Iteration 718 \t| Loss = 0.3943904 \t| Accuracy = 0.921875\n",
      "Iteration 719 \t| Loss = 0.2182576 \t| Accuracy = 0.953125\n",
      "Iteration 720 \t| Loss = 0.27859578 \t| Accuracy = 0.9140625\n",
      "Iteration 721 \t| Loss = 0.1553017 \t| Accuracy = 0.953125\n",
      "Iteration 722 \t| Loss = 0.38901794 \t| Accuracy = 0.8828125\n",
      "Iteration 723 \t| Loss = 0.39994812 \t| Accuracy = 0.8671875\n",
      "Iteration 724 \t| Loss = 0.24028884 \t| Accuracy = 0.9296875\n",
      "Iteration 725 \t| Loss = 0.29466864 \t| Accuracy = 0.9140625\n",
      "Iteration 726 \t| Loss = 0.38048896 \t| Accuracy = 0.8828125\n",
      "Iteration 727 \t| Loss = 0.2324095 \t| Accuracy = 0.9140625\n",
      "Iteration 728 \t| Loss = 0.22471663 \t| Accuracy = 0.9296875\n",
      "Iteration 729 \t| Loss = 0.19532183 \t| Accuracy = 0.9453125\n",
      "Iteration 730 \t| Loss = 0.22981459 \t| Accuracy = 0.9609375\n",
      "Iteration 731 \t| Loss = 0.27721044 \t| Accuracy = 0.9375\n",
      "Iteration 732 \t| Loss = 0.21193925 \t| Accuracy = 0.9296875\n",
      "Iteration 733 \t| Loss = 0.27106458 \t| Accuracy = 0.9375\n",
      "Iteration 734 \t| Loss = 0.3127004 \t| Accuracy = 0.9296875\n",
      "Iteration 735 \t| Loss = 0.4074401 \t| Accuracy = 0.90625\n",
      "Iteration 736 \t| Loss = 0.4234147 \t| Accuracy = 0.875\n",
      "Iteration 737 \t| Loss = 0.27749294 \t| Accuracy = 0.9140625\n",
      "Iteration 738 \t| Loss = 0.40434003 \t| Accuracy = 0.8828125\n",
      "Iteration 739 \t| Loss = 0.41972676 \t| Accuracy = 0.921875\n",
      "Iteration 740 \t| Loss = 0.20409727 \t| Accuracy = 0.953125\n",
      "Iteration 741 \t| Loss = 0.29713324 \t| Accuracy = 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 742 \t| Loss = 0.34893578 \t| Accuracy = 0.90625\n",
      "Iteration 743 \t| Loss = 0.21594423 \t| Accuracy = 0.9296875\n",
      "Iteration 744 \t| Loss = 0.28216323 \t| Accuracy = 0.9140625\n",
      "Iteration 745 \t| Loss = 0.36306486 \t| Accuracy = 0.8828125\n",
      "Iteration 746 \t| Loss = 0.34127718 \t| Accuracy = 0.8828125\n",
      "Iteration 747 \t| Loss = 0.25387275 \t| Accuracy = 0.9609375\n",
      "Iteration 748 \t| Loss = 0.29192513 \t| Accuracy = 0.90625\n",
      "Iteration 749 \t| Loss = 0.3625614 \t| Accuracy = 0.8984375\n",
      "Iteration 750 \t| Loss = 0.30627847 \t| Accuracy = 0.8828125\n",
      "Iteration 751 \t| Loss = 0.25094038 \t| Accuracy = 0.90625\n",
      "Iteration 752 \t| Loss = 0.2278493 \t| Accuracy = 0.9375\n",
      "Iteration 753 \t| Loss = 0.28174424 \t| Accuracy = 0.9140625\n",
      "Iteration 754 \t| Loss = 0.4092275 \t| Accuracy = 0.8671875\n",
      "Iteration 755 \t| Loss = 0.20993772 \t| Accuracy = 0.953125\n",
      "Iteration 756 \t| Loss = 0.17751843 \t| Accuracy = 0.9453125\n",
      "Iteration 757 \t| Loss = 0.2744519 \t| Accuracy = 0.9296875\n",
      "Iteration 758 \t| Loss = 0.35456306 \t| Accuracy = 0.8984375\n",
      "Iteration 759 \t| Loss = 0.27195048 \t| Accuracy = 0.9140625\n",
      "Iteration 760 \t| Loss = 0.28973967 \t| Accuracy = 0.921875\n",
      "Iteration 761 \t| Loss = 0.27185383 \t| Accuracy = 0.9296875\n",
      "Iteration 762 \t| Loss = 0.28419405 \t| Accuracy = 0.921875\n",
      "Iteration 763 \t| Loss = 0.23646286 \t| Accuracy = 0.9296875\n",
      "Iteration 764 \t| Loss = 0.28706306 \t| Accuracy = 0.9375\n",
      "Iteration 765 \t| Loss = 0.28550637 \t| Accuracy = 0.9140625\n",
      "Iteration 766 \t| Loss = 0.20471379 \t| Accuracy = 0.9375\n",
      "Iteration 767 \t| Loss = 0.40738955 \t| Accuracy = 0.8828125\n",
      "Iteration 768 \t| Loss = 0.49986967 \t| Accuracy = 0.875\n",
      "Iteration 769 \t| Loss = 0.37403905 \t| Accuracy = 0.9375\n",
      "Iteration 770 \t| Loss = 0.3810373 \t| Accuracy = 0.9140625\n",
      "Iteration 771 \t| Loss = 0.35655972 \t| Accuracy = 0.8828125\n",
      "Iteration 772 \t| Loss = 0.19130602 \t| Accuracy = 0.9296875\n",
      "Iteration 773 \t| Loss = 0.17997134 \t| Accuracy = 0.9375\n",
      "Iteration 774 \t| Loss = 0.17334995 \t| Accuracy = 0.9609375\n",
      "Iteration 775 \t| Loss = 0.24758731 \t| Accuracy = 0.921875\n",
      "Iteration 776 \t| Loss = 0.13287254 \t| Accuracy = 0.9609375\n",
      "Iteration 777 \t| Loss = 0.37840828 \t| Accuracy = 0.890625\n",
      "Iteration 778 \t| Loss = 0.20758234 \t| Accuracy = 0.9375\n",
      "Iteration 779 \t| Loss = 0.3150727 \t| Accuracy = 0.875\n",
      "Iteration 780 \t| Loss = 0.3571072 \t| Accuracy = 0.90625\n",
      "Iteration 781 \t| Loss = 0.3487864 \t| Accuracy = 0.8984375\n",
      "Iteration 782 \t| Loss = 0.4328032 \t| Accuracy = 0.8828125\n",
      "Iteration 783 \t| Loss = 0.27410784 \t| Accuracy = 0.90625\n",
      "Iteration 784 \t| Loss = 0.44089305 \t| Accuracy = 0.875\n",
      "Iteration 785 \t| Loss = 0.32996628 \t| Accuracy = 0.9375\n",
      "Iteration 786 \t| Loss = 0.22516288 \t| Accuracy = 0.9453125\n",
      "Iteration 787 \t| Loss = 0.35408944 \t| Accuracy = 0.8984375\n",
      "Iteration 788 \t| Loss = 0.30224055 \t| Accuracy = 0.890625\n",
      "Iteration 789 \t| Loss = 0.36209416 \t| Accuracy = 0.890625\n",
      "Iteration 790 \t| Loss = 0.4029124 \t| Accuracy = 0.890625\n",
      "Iteration 791 \t| Loss = 0.44145674 \t| Accuracy = 0.875\n",
      "Iteration 792 \t| Loss = 0.3238982 \t| Accuracy = 0.9453125\n",
      "Iteration 793 \t| Loss = 0.48203847 \t| Accuracy = 0.921875\n",
      "Iteration 794 \t| Loss = 0.20245439 \t| Accuracy = 0.9453125\n",
      "Iteration 795 \t| Loss = 0.472155 \t| Accuracy = 0.8671875\n",
      "Iteration 796 \t| Loss = 0.28314653 \t| Accuracy = 0.90625\n",
      "Iteration 797 \t| Loss = 0.2889133 \t| Accuracy = 0.921875\n",
      "Iteration 798 \t| Loss = 0.2747879 \t| Accuracy = 0.9453125\n",
      "Iteration 799 \t| Loss = 0.22458601 "
     ]
    }
   ],
   "source": [
    "c = Config()\n",
    "m = Mach(c, mnist)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    m.Train(1)\n",
    "    m.Learn(1)\n",
    "    print (i)\n",
    "    if i % 100 == 0:\n",
    "        m.Perf()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.learn_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Config()\n",
    "m = Mach(c, mnist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on test set: 0.9194\n",
      "\n",
      "Accuracy on test set: 0.9234\n",
      "\n",
      "Accuracy on test set: 0.9229\n",
      "\n",
      "Accuracy on test set: 0.9215\n",
      "\n",
      "Accuracy on test set: 0.9231\n",
      "\n",
      "Accuracy on test set: 0.9204\n",
      "\n",
      "Accuracy on test set: 0.9208\n",
      "\n",
      "Accuracy on test set: 0.9215\n",
      "\n",
      "Accuracy on test set: 0.9205\n",
      "\n",
      "Accuracy on test set: 0.9217\n",
      "\n",
      "Accuracy on test set: 0.9207\n",
      "\n",
      "Accuracy on test set: 0.922\n",
      "\n",
      "Accuracy on test set: 0.9194\n",
      "\n",
      "Accuracy on test set: 0.9209\n",
      "\n",
      "Accuracy on test set: 0.9213\n",
      "\n",
      "Accuracy on test set: 0.9189\n",
      "\n",
      "Accuracy on test set: 0.9192\n",
      "\n",
      "Accuracy on test set: 0.9217\n",
      "\n",
      "Accuracy on test set: 0.9209\n",
      "\n",
      "Accuracy on test set: 0.9211\n",
      "\n",
      "Accuracy on test set: 0.9211\n",
      "\n",
      "Accuracy on test set: 0.9217\n",
      "\n",
      "Accuracy on test set: 0.9205\n",
      "\n",
      "Accuracy on test set: 0.9196\n",
      "\n",
      "Accuracy on test set: 0.9214\n",
      "\n",
      "Accuracy on test set: 0.9197\n",
      "\n",
      "Accuracy on test set: 0.9224\n",
      "\n",
      "Accuracy on test set: 0.9207\n",
      "\n",
      "Accuracy on test set: 0.9202\n",
      "\n",
      "Accuracy on test set: 0.9209\n",
      "\n",
      "Accuracy on test set: 0.9212\n",
      "\n",
      "Accuracy on test set: 0.9206\n",
      "\n",
      "Accuracy on test set: 0.9217\n",
      "\n",
      "Accuracy on test set: 0.921\n",
      "\n",
      "Accuracy on test set: 0.9196\n",
      "\n",
      "Accuracy on test set: 0.9217\n",
      "\n",
      "Accuracy on test set: 0.92\n",
      "\n",
      "Accuracy on test set: 0.9234\n",
      "\n",
      "Accuracy on test set: 0.9216\n",
      "\n",
      "Accuracy on test set: 0.9204\n",
      "\n",
      "Accuracy on test set: 0.9212\n",
      "\n",
      "Accuracy on test set: 0.9213\n",
      "\n",
      "Accuracy on test set: 0.9206\n",
      "\n",
      "Accuracy on test set: 0.9198\n",
      "\n",
      "Accuracy on test set: 0.9215\n",
      "\n",
      "Accuracy on test set: 0.9188\n",
      "\n",
      "Accuracy on test set: 0.9203\n",
      "\n",
      "Accuracy on test set: 0.92\n",
      "\n",
      "Accuracy on test set: 0.9216\n",
      "\n",
      "Accuracy on test set: 0.9209\n",
      "\n",
      "Accuracy on test set: 0.9209\n",
      "\n",
      "Accuracy on test set: 0.9209\n",
      "\n",
      "Accuracy on test set: 0.9221\n",
      "\n",
      "Accuracy on test set: 0.9202\n",
      "\n",
      "Accuracy on test set: 0.9214\n",
      "\n",
      "Accuracy on test set: 0.9215\n",
      "\n",
      "Accuracy on test set: 0.9195\n",
      "\n",
      "Accuracy on test set: 0.9217\n",
      "\n",
      "Accuracy on test set: 0.9206\n",
      "\n",
      "Accuracy on test set: 0.9206\n",
      "\n",
      "Accuracy on test set: 0.9215\n",
      "\n",
      "Accuracy on test set: 0.9217\n",
      "\n",
      "Accuracy on test set: 0.919\n",
      "\n",
      "Accuracy on test set: 0.9211\n",
      "\n",
      "Accuracy on test set: 0.9207\n",
      "\n",
      "Accuracy on test set: 0.921\n",
      "\n",
      "Accuracy on test set: 0.9202\n",
      "\n",
      "Accuracy on test set: 0.9206\n",
      "\n",
      "Accuracy on test set: 0.9209\n",
      "\n",
      "Accuracy on test set: 0.9216\n",
      "\n",
      "Accuracy on test set: 0.9203\n",
      "\n",
      "Accuracy on test set: 0.9212\n",
      "\n",
      "Accuracy on test set: 0.9205\n",
      "\n",
      "Accuracy on test set: 0.9201\n",
      "\n",
      "Accuracy on test set: 0.9213\n",
      "\n",
      "Accuracy on test set: 0.9213\n",
      "\n",
      "Accuracy on test set: 0.9197\n",
      "\n",
      "Accuracy on test set: 0.9206\n",
      "\n",
      "Accuracy on test set: 0.9202\n",
      "\n",
      "Accuracy on test set: 0.9201\n",
      "\n",
      "Accuracy on test set: 0.92\n",
      "\n",
      "Accuracy on test set: 0.921\n",
      "\n",
      "Accuracy on test set: 0.9206\n",
      "\n",
      "Accuracy on test set: 0.9189\n",
      "\n",
      "Accuracy on test set: 0.9203\n",
      "\n",
      "Accuracy on test set: 0.9199\n",
      "\n",
      "Accuracy on test set: 0.922\n",
      "\n",
      "Accuracy on test set: 0.9223\n",
      "\n",
      "Accuracy on test set: 0.9207\n",
      "\n",
      "Accuracy on test set: 0.9208\n",
      "\n",
      "Accuracy on test set: 0.9206\n",
      "\n",
      "Accuracy on test set: 0.9211\n",
      "\n",
      "Accuracy on test set: 0.9203\n",
      "\n",
      "Accuracy on test set: 0.9203\n",
      "\n",
      "Accuracy on test set: 0.9189\n",
      "\n",
      "Accuracy on test set: 0.922\n",
      "\n",
      "Accuracy on test set: 0.92\n",
      "\n",
      "Accuracy on test set: 0.9198\n",
      "\n",
      "Accuracy on test set: 0.921\n",
      "\n",
      "Accuracy on test set: 0.9202\n",
      "\n",
      "Accuracy on test set: 0.9197\n",
      "\n",
      "Accuracy on test set: 0.9202\n",
      "\n",
      "Accuracy on test set: 0.9204\n",
      "\n",
      "Accuracy on test set: 0.9205\n",
      "\n",
      "Accuracy on test set: 0.9209\n",
      "\n",
      "Accuracy on test set: 0.9202\n",
      "\n",
      "Accuracy on test set: 0.9205\n",
      "\n",
      "Accuracy on test set: 0.9201\n",
      "\n",
      "Accuracy on test set: 0.9193\n",
      "\n",
      "Accuracy on test set: 0.9196\n",
      "\n",
      "Accuracy on test set: 0.9203\n",
      "\n",
      "Accuracy on test set: 0.9208\n",
      "\n",
      "Accuracy on test set: 0.9209\n",
      "\n",
      "Accuracy on test set: 0.9203\n",
      "\n",
      "Accuracy on test set: 0.92\n",
      "\n",
      "Accuracy on test set: 0.9186\n",
      "\n",
      "Accuracy on test set: 0.9198\n",
      "\n",
      "Accuracy on test set: 0.9206\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c18b2c6be65b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-7825f6279fce>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mfeeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m#             # print loss and accuracy (per minibatch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    m2.Train()\n",
    "    m.Train()\n",
    "    m.Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
